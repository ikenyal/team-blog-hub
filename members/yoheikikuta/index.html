<!DOCTYPE html><html><head><meta name="viewport" content="width=device-width"/><meta charSet="utf-8"/><link rel="icon shortcut" type="image/png" href="https://blog.ubie.tech/logo.png"/><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Inter:wght@400;700&amp;display=swap"/><title>yoheikikuta | Ubie Engineers&#x27; Blogs</title><meta property="og:title" content="yoheikikuta"/><meta property="og:url" content="https://blog.ubie.tech/members/yoheikikuta"/><meta name="twitter:card" content="summary_large_image"/><meta property="og:site" content="Ubie Engineers&#x27; Blogs"/><meta property="og:image" content="https://blog.ubie.tech/og.png"/><link rel="canonical" href="https://blog.ubie.tech/members/yoheikikuta"/><link rel="preload" href="/_next/static/css/5b1e5c056c67d836f701.css" as="style"/><link rel="stylesheet" href="/_next/static/css/5b1e5c056c67d836f701.css" data-n-g=""/><noscript data-n-css="true"></noscript><link rel="preload" href="/_next/static/chunks/main-8a83f0fd99327c4684a8.js" as="script"/><link rel="preload" href="/_next/static/chunks/webpack-e067438c4cf4ef2ef178.js" as="script"/><link rel="preload" href="/_next/static/chunks/framework.1daf1ec1ecf144ee9147.js" as="script"/><link rel="preload" href="/_next/static/chunks/commons.8d61253ae98ee51657b8.js" as="script"/><link rel="preload" href="/_next/static/chunks/pages/_app-49079e3278dd6cef7229.js" as="script"/><link rel="preload" href="/_next/static/chunks/81b50c7ab23905e464b4340eb234bd6ea389d26b.1ba37b316530e91fa3ed.js" as="script"/><link rel="preload" href="/_next/static/chunks/pages/members/%5Bid%5D-f279413a3daf3c18264d.js" as="script"/></head><body><div id="__next"><header class="site-header"><div class="content-wrapper"><div class="site-header__inner"><a class="site-header__logo-link" href="/"><img src="/logo.png" alt="Ubie Engineers&#x27; Blogs" class="site-header__logo-img"/><span class="site-header__logo-text">Ubie<br/>Engineers&#x27; Blogs</span></a><div class="site-header__links"><a href="https://ubie.life/" class="site-header__link" target="_blank">Company</a><a href="https://recruit.ubie.life/jd_dev" class="site-header__link" target="_blank">Recruit</a></div></div></div></header><section class="member"><div class="content-wrapper"><header class="member-header"><div class="member-header__avatar"><img src="/avatars/yoheikikuta.jpg" alt="yoheikikuta" width="100" height="100" class="member-header__avatar-img"/></div><h1 class="member-header__nickname">yoheikikuta</h1><p class="member-header__real-name">Yohei Kikuta</p><p class="member-header__bio">learning machine learning</p><div class="member-header__links"><a href="https://twitter.com/yohei_kikuta" class="member-header__link"><img src="/icons/twitter.svg" alt="Twitterのユーザー@yohei_kikuta" width="22" height="22"/></a><a href="https://github.com/yoheikikuta" class="member-header__link"><img src="/icons/github.svg" alt="GitHubのユーザー@yoheikikuta" width="22" height="22"/></a><a href="https://github.com/yoheikikuta/resume" class="member-header__link"><img src="/icons/link.svg" alt="ウェブサイトのリンク" width="22" height="22"/></a></div></header><div class="member-posts-container"><div class="post-list"><article class="post-link"><a class="post-link__author" href="/members/yoheikikuta/"><img src="/avatars/yoheikikuta.jpg" class="post-link__author-img" width="35" height="35"/><div class="post-link__author-name"><div class="post-link__author-name">yoheikikuta</div><time dateTime="2021-11-26T00:00:00.000Z" class="post-link__date">a month ago</time></div></a><a href="https://yoheikikuta.github.io/BigQuery_tips_part3/" class="post-link__main-link"><h2 class="post-link__title">BigQuery を使って分析する際の tips (part3)</h2><div class="post-link__site"><img src="https://www.google.com/s2/favicons?domain=yoheikikuta.github.io" width="14" height="14" class="post-link__site-favicon"/>yoheikikuta.github.io</div></a></article><article class="post-link"><a class="post-link__author" href="/members/yoheikikuta/"><img src="/avatars/yoheikikuta.jpg" class="post-link__author-img" width="35" height="35"/><div class="post-link__author-name"><div class="post-link__author-name">yoheikikuta</div><time dateTime="2021-11-21T00:00:00.000Z" class="post-link__date">a month ago</time></div></a><a href="https://yoheikikuta.github.io/BigQuery_tips_part2/" class="post-link__main-link"><h2 class="post-link__title">BigQuery を使って分析する際の tips (part2)</h2><div class="post-link__site"><img src="https://www.google.com/s2/favicons?domain=yoheikikuta.github.io" width="14" height="14" class="post-link__site-favicon"/>yoheikikuta.github.io</div></a></article><article class="post-link"><a class="post-link__author" href="/members/yoheikikuta/"><img src="/avatars/yoheikikuta.jpg" class="post-link__author-img" width="35" height="35"/><div class="post-link__author-name"><div class="post-link__author-name">yoheikikuta</div><time dateTime="2021-11-13T00:00:00.000Z" class="post-link__date">2 months ago</time></div></a><a href="https://yoheikikuta.github.io/BigQuery_tips_part1/" class="post-link__main-link"><h2 class="post-link__title">BigQuery を使って分析する際の tips (part1)</h2><div class="post-link__site"><img src="https://www.google.com/s2/favicons?domain=yoheikikuta.github.io" width="14" height="14" class="post-link__site-favicon"/>yoheikikuta.github.io</div></a></article><article class="post-link"><a class="post-link__author" href="/members/yoheikikuta/"><img src="/avatars/yoheikikuta.jpg" class="post-link__author-img" width="35" height="35"/><div class="post-link__author-name"><div class="post-link__author-name">yoheikikuta</div><time dateTime="2021-10-26T00:00:00.000Z" class="post-link__date">2 months ago</time></div></a><a href="https://yoheikikuta.github.io/casual_talk_launchable/" class="post-link__main-link"><h2 class="post-link__title">Launchable の人とカジュアル面談した</h2><div class="post-link__site"><img src="https://www.google.com/s2/favicons?domain=yoheikikuta.github.io" width="14" height="14" class="post-link__site-favicon"/>yoheikikuta.github.io</div></a></article><article class="post-link"><a class="post-link__author" href="/members/yoheikikuta/"><img src="/avatars/yoheikikuta.jpg" class="post-link__author-img" width="35" height="35"/><div class="post-link__author-name"><div class="post-link__author-name">yoheikikuta</div><time dateTime="2021-10-01T00:00:00.000Z" class="post-link__date">3 months ago</time></div></a><a href="https://yoheikikuta.github.io/ml_design_book/" class="post-link__main-link"><h2 class="post-link__title">施策デザインのための機械学習入門 を読んだ</h2><div class="post-link__site"><img src="https://www.google.com/s2/favicons?domain=yoheikikuta.github.io" width="14" height="14" class="post-link__site-favicon"/>yoheikikuta.github.io</div></a></article><article class="post-link"><a class="post-link__author" href="/members/yoheikikuta/"><img src="/avatars/yoheikikuta.jpg" class="post-link__author-img" width="35" height="35"/><div class="post-link__author-name"><div class="post-link__author-name">yoheikikuta</div><time dateTime="2021-05-19T00:00:00.000Z" class="post-link__date">7 months ago</time></div></a><a href="https://yoheikikuta.github.io/my_purpose_of_DS_and_ML/" class="post-link__main-link"><h2 class="post-link__title">自分がデータ分析/機械学習で成し遂げたいこと</h2><div class="post-link__site"><img src="https://www.google.com/s2/favicons?domain=yoheikikuta.github.io" width="14" height="14" class="post-link__site-favicon"/>yoheikikuta.github.io</div></a></article><article class="post-link"><a class="post-link__author" href="/members/yoheikikuta/"><img src="/avatars/yoheikikuta.jpg" class="post-link__author-img" width="35" height="35"/><div class="post-link__author-name"><div class="post-link__author-name">yoheikikuta</div><time dateTime="2021-05-02T00:00:00.000Z" class="post-link__date">8 months ago</time></div></a><a href="https://yoheikikuta.github.io/lookback_on_hikifunefm/" class="post-link__main-link"><h2 class="post-link__title">hikifunefm 反省会会場</h2><div class="post-link__site"><img src="https://www.google.com/s2/favicons?domain=yoheikikuta.github.io" width="14" height="14" class="post-link__site-favicon"/>yoheikikuta.github.io</div></a></article><article class="post-link"><a class="post-link__author" href="/members/yoheikikuta/"><img src="/avatars/yoheikikuta.jpg" class="post-link__author-img" width="35" height="35"/><div class="post-link__author-name"><div class="post-link__author-name">yoheikikuta</div><time dateTime="2021-02-22T00:00:00.000Z" class="post-link__date">10 months ago</time></div></a><a href="https://yoheikikuta.github.io/GAS_local_development_env/" class="post-link__main-link"><h2 class="post-link__title">clasp と TypeScript で GAS の開発環境を整えた備忘録</h2><div class="post-link__site"><img src="https://www.google.com/s2/favicons?domain=yoheikikuta.github.io" width="14" height="14" class="post-link__site-favicon"/>yoheikikuta.github.io</div></a></article><article class="post-link"><a class="post-link__author" href="/members/yoheikikuta/"><img src="/avatars/yoheikikuta.jpg" class="post-link__author-img" width="35" height="35"/><div class="post-link__author-name"><div class="post-link__author-name">yoheikikuta</div><time dateTime="2020-12-30T00:00:00.000Z" class="post-link__date">a year ago</time></div></a><a href="https://yoheikikuta.github.io/Summary2020/" class="post-link__main-link"><h2 class="post-link__title">2020年のまとめ</h2><div class="post-link__site"><img src="https://www.google.com/s2/favicons?domain=yoheikikuta.github.io" width="14" height="14" class="post-link__site-favicon"/>yoheikikuta.github.io</div></a></article><article class="post-link"><a class="post-link__author" href="/members/yoheikikuta/"><img src="/avatars/yoheikikuta.jpg" class="post-link__author-img" width="35" height="35"/><div class="post-link__author-name"><div class="post-link__author-name">yoheikikuta</div><time dateTime="2020-12-29T00:00:00.000Z" class="post-link__date">a year ago</time></div></a><a href="https://yoheikikuta.github.io/book_I_read_in_2020/" class="post-link__main-link"><h2 class="post-link__title">2020年に読んだ本を一言コメントと共に振り返る</h2><div class="post-link__site"><img src="https://www.google.com/s2/favicons?domain=yoheikikuta.github.io" width="14" height="14" class="post-link__site-favicon"/>yoheikikuta.github.io</div></a></article></div></div></div></section><footer class="site-footer"><div class="content-wrapper"><p>© <!-- -->Ubie Discovery</p></div></footer></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"member":{"id":"yoheikikuta","nickname":"yoheikikuta","realName":"Yohei Kikuta","bio":"learning machine learning","avatarSrc":"/avatars/yoheikikuta.jpg","sources":["https://yoheikikuta.github.io/feed.xml"],"twitterUsername":"yohei_kikuta","githubUsername":"yoheikikuta","websiteUrl":"https://github.com/yoheikikuta/resume"},"postItems":[{"title":"BigQuery を使って分析する際の tips (part3)","contentSnippet":"TL;DRpart3 は STRUCT/ARRAY 型の理解、便利機能、preview 機能について書くBigQuery は分析に便利な機能がたくさんあって楽しいBigQuery で分析する際の tips シリーズは一旦これで終了part3 では part2 よりももっと具体的なクエリを対象として tips をまとめる。具体的なクエリになるほど公式リファレンスを見れば分かるというものになりがちだが、その中でも個人的にこの辺を押さえておくとよさそうというものについて書くことにする。tips7: STRUCT 型と ARRAY 型を使いこなすpart2 のスカラー関数・集計関数・分析関数のところでも触れたが、BigQuery で分析する際に STRUCT 型や ARRAY 型はよく使うので、これらに関してはスムースに読み書きできるようにしておくと役に立つ。と言っても STRUCT 型の場合にはそれほど気を付けることはないが、NULL の判定は注意が必要。STRUCT 自体が NULL なのかその中の field が NULL なのかは明確に区別する必要があり、前者は単に struct_column IS NULL のように比較すればよいが、後者は以下の例で分かるように STRUCT リテラルでは比較できないので field アクセスして NULL 比較せねばならない。SELECT IF((1,\"a\") = (1,\"a\"),1,0) AS resultUNION ALL SELECT IF((1,NULL) = (1,NULL),1,0)UNION ALL SELECT IF((NULL,NULL) = (NULL,NULL),1,0)UNION ALL SELECT IF(STRUCT(NULL AS c1,NULL AS c2).c1 IS NULL,1,0)行result11203041ARRAY 型の場合も ARRAY 自体が NULL なのか中身が空の配列なのかを区別する必要があり、前者は単に array_column IS NULL のように比較すればよいが、後者は ARRAY_LENGTH(array_column) = 0 のように指定する必要がある。これらの違いは BigQuery console 上で見ても違いが分からないので、ちゃんと認識しておかないと想定外のミスをしてしまう可能性がある。ちなみに ARRAY 型は配列の要素に NULL を持つことは禁止されている。また、イコールで比較する機能は提供されていない。WITH  test AS (  SELECT [1,2,3] AS array_column  UNION ALL SELECT []  UNION ALL SELECT NULL)SELECT  array_column,  array_column IS NULL AS is_null_array_column,  ARRAY_LENGTH(array_column) = 0 AS is_zero_length_array_columnFROM  test行array_columnis_null_array_columnis_zero_length_array_column11falsefalse232falsetrue3truenull次に ARRAY 型を扱う上で最も重要になる UNNEST について。UNNEST を最もよく使うのは ARRAY を解いて元のソーステーブルに JOIN して使うケースだろう。UNNEST を使う場合 correlated join について一度は公式リファレンス https://cloud.google.com/bigquery/docs/reference/standard-sql/query-syntax#correlated_join に目を通しておくのがよいと思う。その上で以下のように具体的な例で振る舞いを把握しておくのがよい。WITH  sequences AS (  SELECT    1 AS id,    [1,2,3] AS some_numbers  UNION ALL SELECT    2 AS id,    [2,4] AS some_numbers  UNION ALL SELECT    3 AS id,    [3] AS some_numbers  UNION ALL SELECT    4 AS id,    [] AS some_numbers)SELECT  id,  flattened_numbersFROM  sequences,UNNEST(sequences.some_numbers) AS flattened_numbers----- 以下全部同じ振る舞い--   sequences CROSS JOIN UNNEST(sequences.some_numbers) AS flattened_numbers--   sequences, sequences.some_numbers AS flattened_numbers--   sequences INNER JOIN UNNEST(sequences.some_numbers) AS flattened_numbers----- UNNEST する ARRAY において NULL ARRAY や要素が空のものも残す場合は LEFT JOIN--   sequences LEFT JOIN UNNEST(sequences.some_numbers) AS flattened_numbers行idflattened_numbers111212313422524633UNNEST は IN UNNEST(foo) などでも使えてこれは単に配列をバラしてスキャンするだけだが、この correlated join の場合は単に配列をバラしてるだけではなくて特別な振る舞いをするものなので慣れが必要である。これはもうあれこれ考えるのではなくこういうもんだとして慣れて、それを前提知識とするのが一番いいかなと思っている（脳筋だが、それくらいよく使うものなので）。ちょっと脱線だが、取り出したい要素が一行に対して一つの時はスカラーサブクエリの中で UNNEST を使うこともできる。例えば先ほどの例で ARRAY の中身の MAX だけを取り出したければ以下のように書けるけど、UNNEST とスカラーサブクエリが重なって複雑になって読みにくいので個人的にはあまり好きではない。素直に correlated join して GROUP BY とかする方が好み。WITH  sequences AS (  SELECT    1 AS id,    [1,2,3] AS some_numbers  UNION ALL SELECT    2 AS id,    [2,4] AS some_numbers  UNION ALL SELECT    3 AS id,    [3] AS some_numbers  UNION ALL SELECT    4 AS id,    null AS some_numbers)SELECT  id,  (SELECT MAX(flattened_numbers) FROM UNNEST(sequences.some_numbers) AS flattened_numbers) AS max_some_numbersFROM  sequences行idmax_some_numbers11322433344nullARRAY は順序付きリストでかつ OFFSET を使って要素にアクセスしてスカラー関数のように使うこともできるので、特定の位置の要素のみに興味がある場合は UNNEST を使わずに望むクエリを書くこともできる。例えば ARRAY に入っている最初の要素だけ取り出せればいいというのであれば、以下のように書けばよい。ただしこれは対象としている ARRAY が何かしらのルールでちゃんと sort されていることに依存しているので、それが保証されてない限りはちゃんと UNNEST して ORDER BY するなりしてクエリを書くべき。ここでの話とは直接関係しないが、UNNEST すると順序が保証されなくなる https://cloud.google.com/bigquery/docs/reference/standard-sql/arrays#flattening_arrays ので注意。WITH  sequences AS (  SELECT    1 AS id,    [1,2,3] AS some_numbers  UNION ALL SELECT    2 AS id,    [2,4] AS some_numbers  UNION ALL SELECT    3 AS id,    [3] AS some_numbers  UNION ALL SELECT    4 AS id,    null AS some_numbers)SELECT  id,  some_numbers[OFFSET(0)] AS first_elem_some_numbersFROM  sequences行idfirst_elem_some_numbers11122233344null最後に GENERATE_ARRAY 系に触れておく。BigQuery には連続した要素を持つ ARRAY を生成する便利な関数が存在する。公式リファレンスは https://cloud.google.com/bigquery/docs/reference/standard-sql/array_functions#generate_array この辺り。例えば、ログを集計して日毎のエラー数を出す時に、エラーがない日があると日付が歯抜けになってしまうのでダッシュボードなどで使う時にちょっと不便だったりすることがあり得る。そういう場合には例えば以下のように GENERATE_DATE_ARRAY 関数を使ってテーブルを生成し、これに対して LEFT JOIN するなどすればよい。WITH  base_date AS (  SELECT    generated_date  FROM    UNNEST(GENERATE_DATE_ARRAY(DATE_SUB(CURRENT_DATE(), INTERVAL 14 DAY), CURRENT_DATE())) AS generated_date )...ARRAY を使いこなすと分析でだいぶ自由が効くようになるので、バンバン使っていきましょう。tips8: いくつかの便利機能紹介雑多な感じにはなるが、クエリを書くときの便利機能をいくつか紹介する。UDF公式リファレンスは https://cloud.google.com/bigquery/docs/reference/standard-sql/user-defined-functionsUDF は JavaScript が使えるので、分析を BigQuery だけで完結させたいという場合に有効である。そんなに面白い例ではないが、例えば以下のようにフィボナッチ数を計算することができたりする。CREATE TEMPORARY FUNCTION  fibonacchi( num INT64 )  RETURNS INT64  LANGUAGE js AS \"function fibo(n) {return n \u003c 2 ? n : fibo(n - 2) + fibo(n - 1);} return fibo(num);\";WITH num_table AS (  SELECT num FROM UNNEST(GENERATE_ARRAY(1,10,1)) AS num)SELECT  num,  fibonacchi(num) AS resultFROM  num_table 行numresult111221332443555668771388219934101055SQL だけだと表現しにくい数理的な演算やちょっとした自然言語処理をしたい場合に JavaScript UDF は便利。JavaScript のコードが長くなるという場合には GCS にファイルを置いて読み込む https://cloud.google.com/bigquery/docs/reference/standard-sql/user-defined-functions#including-javascript-libraries こともできる。（今回のシリーズでは気にしないと言ったが）パフォーマンスは犠牲になる https://cloud.google.com/bigquery/docs/best-practices-performance-compute#avoid_javascript_user-defined_functions ので注意。現在は JavaScript のみ対応してるが、最近の Google Cloud Next で他の言語への拡張が話題に挙がっていたので期待大。https://www.youtube.com/watch?v=MY2vBrjA_xg\u0026t=367sチームで同じような処理を使い回す場合にも UDF は便利だが、その場合は適当に作りまくると管理が大変になりがちなので注意。会社だと Terraform で管理してるが、今回の主題からは離れているので割愛。JSON functionsBigQuery は JSON データを扱うための便利な関数も提供してくれている。公式リファレンスは https://cloud.google.com/bigquery/docs/reference/standard-sql/json_functionsここで一番言いたいのは、JSON_EXTRACT を使っている人は JSON_QUERY や JSON_VALUE を使うようにしようということだ（公式が推奨してるので）。実際のところこれらの違いは大きくはないが、JSON_EXTRACT では JSONPath で invalid な文字が出てきた時に single quote と [] を使ってエスケープする必要がある。JSON_QUERY などの場合は [] を使わずに double quote を使うことができる。SELECT JSON_QUERY('{\"a.b\": {\"c\": \"hello world\"}}', '$.\"a.b\"') AS testUNION ALL SELECT JSON_VALUE('{\"a.b\": {\"c\": \"hello world\"}}', '$.\"a.b\"')UNION ALL SELECT JSON_QUERY('{\"a.b\": {\"c\": \"hello world\"}}', '$.\"a.b\".c')UNION ALL SELECT JSON_VALUE('{\"a.b\": {\"c\": \"hello world\"}}', '$.\"a.b\".c')-- UNION ALL SELECT JSON_EXTRACT('{\"a.b\": {\"c\": \"hello world\"}}', '$.\"a.b\".c') AS test  -- これはエラー-- UNION ALL SELECT JSON_EXTRACT_SCALAR('{\"a.b\": {\"c\": \"hello world\"}}', '$.\"a.b\".c') AS test  -- これはエラーUNION ALL SELECT JSON_EXTRACT('{\"a.b\": {\"c\": \"hello world\"}}', \"$['a.b']\")UNION ALL SELECT JSON_EXTRACT_SCALAR('{\"a.b\": {\"c\": \"hello world\"}}', \"$['a.b']\")UNION ALL SELECT JSON_EXTRACT('{\"a.b\": {\"c\": \"hello world\"}}', \"$['a.b'].c\")UNION ALL SELECT JSON_EXTRACT_SCALAR('{\"a.b\": {\"c\": \"hello world\"}}', \"$['a.b'].c\")行test1{\"c\":\"hello world\"}2null3\"hello world\"4hello world5{\"c\":\"hello world\"}6null7\"hello world\"8hello world他にも JSON-formatted string から ARRAY を抜き出す JSON_QUERY_ARRAY や JSON_VALUE_ARRAY が存在する。例えば、{\"a\": [1,2,3,\"abc\"]} という STRING から ARRAY を抜き出し（各要素は STRING となる）、その各要素を使って concat して一つの STRING にするみたいなのは以下のようにスッと書ける。SELECT  ARRAY_TO_STRING(arr, \"-\") AS testFROM (SELECT JSON_VALUE_ARRAY('{\"a\": [1,2,3,\"abc\"]}','$.a') AS arr)行test11-2-3-abcJSON データを分析する場合は JSON functions の公式リファレンスを一通り眺めてからやるとだいぶ見通しがよくなるだろう。Scripting statementsScripting は一度のリクエストで複数個のステートメントを実行できる機能である。公式リファレンスは https://cloud.google.com/bigquery/docs/reference/standard-sql/scriptingその機能からして分析で使いたいものはあまりないのだけど、自分は対象を変えながら使い回しをするクエリ DECLARE を使ったりする。例えば、以下のようなクエリを手元に持っておいて、必要に応じて target_corpus の要素だけを書き換えて実行する。DECLARE target_corpus ARRAY\u003cSTRING\u003e DEFAULT [\"sonnets\",\"various\",\"1kinghenryvi\",\"2kinghenryvi\",\"3kinghenryvi\"];SELECT  word,  COUNT(1) as cntFROM  `bigquery-public-data.samples.shakespeare`WHERE   corpus IN UNNEST(target_corpus)GROUP BY   wordORDER BY  cnt desc, word行wordcnt1'52'tis53A54Ah55Alas5.........これは TEMPORARY FUNCTION を使っても同じようなことができるけど、文法的に変数であることが明確なので自分はこっちの方が好み。ただし、複数個のステートメントの実行は BigQuery コンソールじゃないと対応できないことが多い（20211127 時点で DataGrip ではまだサポートされてない）ので、泣く泣く WITH 句を使って書いたりすることもあり、他の環境でもサポートされるといいなぁと常々思っている。Scripting は自分は使ってるものがあまりないけど、公式リファレンスを眺めると色々できることがあるというので面白い。tips9: preview 機能を楽しむ最後は 20211127 時点で preview である機能の中で面白そうなものを眺めてみることにする。まずは QUALIFY 句。公式リファレンスは https://cloud.google.com/bigquery/docs/reference/standard-sql/query-syntax#qualify_clauseこれは分析関数のフィルタリングができるというとても便利な機能で、例えば以下のように使用できる。SELECT  word,  word_count,  LENGTH(word) / MAX(LENGTH(word)) OVER () AS ratio_word_lengthFROM  `bigquery-public-data.samples.shakespeare`WHERE   TRUE QUALIFY   ratio_word_length \u003e 0.6ORDER BY  ratio_word_length DESC, word行wordword_countratio_word_length1honorificabilitudinitatibus11.02Anthropophaginian10.62962962962962973indistinguishable10.62962962962962974undistinguishable20.6296296296296297これまでは WITH 句を使って多段で処理したりする必要があったが、それがすっきりと書けるのでこれはぜひ GA になって欲しい。と言いたいところだが、上のクエリには無意味な WHERE TRUE が入ってることにお気づきだろうか。QUALIFY を使うには WHERE もしくは GROUP BY もしくは HAVING の少なくとも一つが含まれている必要があり、望むクエリを書く際にこれらを使う必要がない場合は、上記のように WHERE TRUE のような虚無を挿入しなければならない、という制約があるためである。そしてこれは an implementation limitation ですと説明されている…悲しい…それでも GA にはなって欲しいが。次は table sampling で、これは大きい（1 GB 以上）テーブルのランダムなサブセットを取得することができる機能である。公式リファレンスは https://cloud.google.com/bigquery/docs/table-samplingBigQuery は LIMIT で件数を区切ってもデータスキャン量は変わらないが、以下のように書くことでデータスキャン量が変わっていることが確認できる。デカいテーブルに対して色々クエリを投げながら分析する時に、クエリが固まるまではサンプリングして試せば無駄な課金を減らしたり実行時間を早くしたりできる。SELECT  *FROM  `bigquery-public-data.samples.wikipedia` TABLESAMPLE SYSTEM (1 PERCENT)LIMIT 10（結果は deterministic ではなくキャッシュもされないので実行ごとに変わり得る）新しい機能好きな人はちょくちょく Release notes https://cloud.google.com/bigquery/docs/release-notes などをチェックしておくと楽しいかもしれない。まとめBigQuery を使って分析する際の tips part1~3 を書いた。part3 は公式リファレンスが一番、みたいな話になってしまった…part1: 開発環境やデータ連携 https://yoheikikuta.github.io/BigQuery_tips_part1part2: クエリを書く際に押さえておくとよいこと https://yoheikikuta.github.io/BigQuery_tips_part2part3: 具体的なクエリの tips （このエントリ）https://yoheikikuta.github.io/BigQuery_tips_part3","link":"https://yoheikikuta.github.io/BigQuery_tips_part3/","isoDate":"2021-11-26T00:00:00.000Z","dateMiliSeconds":1637884800000,"authorName":"yoheikikuta","authorId":"yoheikikuta"},{"title":"BigQuery を使って分析する際の tips (part2)","contentSnippet":"TL;DRpart2 はスカラー関数・集計関数・分析関数、サブクエリ、型変換について書くBigQuery は便利な機能が色々備わってるので、それらの基本的な振る舞いを頭に入れておくと便利本文と全然関係ないけど自分のブログはコードブロックの表示とかイマイチなので改善せねばか…part1 に続いて part2 として、分析する際によく使うことになる道具について理解しておくとよいことをいくつかピックアップしてまとめる。ちなみに今回の tips シリーズではクエリのパフォーマンスは気にしない。自分が現状仕事で書いてるものはほぼクエリのパフォーマンスを気にしなくてよいのと、そもそも BigQuery が強力なので細かいことを気にせずに書いてしまって殆どの場合問題ない、というのが理由。実行前のデータスキャン量だけ見ておいて、数百 [GB] 以上のクエリをガンガン実行しそうとなったらコストを気にし始める、というくらいしておけば自分の環境では十分という状況である。tips4: スカラー関数・集計関数・分析関数分析でよく使う関数といえば集計関数や分析関数だが、これらの振る舞いを理解しておくために合わせてスカラー関数も見ておく。これらの関数の振る舞いとして、入力行と出力行の対応が以下のようになっているというイメージを念頭に入れておくとよい。スカラー関数 (入力行):(出力行) = 1:1集計関数 (入力行):(出力行) = N:1分析関数 (入力行):(出力行) = N:Nスカラー関数スカラー関数は入力行と出力行が 1:1 になるもので、例えば以下のクエリは word column の各行に対して最初の文字から 2 文字目までを取得するクエリになっている。SELECT  word,  SUBSTR(word, 0, 2) AS substr_wordFROM  `bigquery-public-data.samples.shakespeare`ORDER BY  word DESC行wordsubstr_word1zwaggeredzw2zoundszo3zonezo.........これは特別何かを意識する必要はないものではあるが、リファレンスなどを呼んでてスカラー関数と出てきたときはこのような振る舞いをするものだと認識しておけばよい。スカラー関数を殊更に意識するところはそんなにないが、スカラー関数にはたいてい SAFE. 接頭辞が利用可能で、これをつけるとエラーを発生させずに NULL を返すものに変えることができる。上の例でいうと、SUBSTR(word, 0, -2) とすると Third argument in SUBSTR() cannot be negative というエラーが発生するが、SAFE.SUBSTR(word, 0, -2) とすれば各行で NULL が返る（この例ではあまり意味が感じられないが、引数が何かの計算よって得られるもので、それが負になりうるケースがあるという場合には使えるかもしれない）。脱線になるが、この SAFE. という接頭辞は演算子には使えない。分析上よく遭遇するエラーとして division by zero があるが、これを防ごうとして 1 SAFE./ 0 などとすることはできない。そのため、演算子と等価でありエラーが出るときには NULL を返す SAFE_xxx スカラー関数が準備されており、除算の場合は SAFE_DIVIDE を使うことになる（なお、整数除算の場合は演算子がなくスカラー関数 DIV があるのみなので、こちらの場合は SAFE.DIV とすればよい）。さらに脱線になるが、この SAFE. というのは関数の返り値を評価するときに使われるもので、引数の式を評価するときには使われないものであると頭に入れておくと、SAFE. が使えないスカラー関数がいるということも理解しやすい。例えば LOWER スカラー関数は STRING を引数にして、STRING であればエラーは出ないので SAFE.LOWER はサポートされていない。Lower(1) のようなものは引数の式を評価して型がマッチしないということで実行前にエラーが分かるためで、型の静的解析でエラーか分かるものは SAFE. とかいらなくて実行時に初めてエラーか分かるものには SAFE. が使えるみたいなイメージ。ついつい SAFE. の話をしてしまったが、ともかくスカラー関数は (入力行):(出力行) = 1:1 という関係で素直に使えばよい。集計関数集計関数に関する公式リファレンスは https://cloud.google.com/bigquery/docs/reference/standard-sql/aggregate_functions である。集計関数は入力行と出力行が N:1 になるもので、例えば以下のクエリは word_count の DISTINCT COUNT を計算するクエリである。SELECT  COUNT(DISTINCT word_count)FROM  `bigquery-public-data.samples.shakespeare`行distinct_count_word_count1483複数行（ここでは全ての行）に対して集計した結果を1行で返しているというもので、特に難しいところはない。GROUP BY で集計するグループを設定するというのも普段から自然にやっている人が多いだろう、例えば以下のような感じ（集計関数の引数に入れてる LIMIT は単に結果が長くなりすぎるので切っているだけ）。SELECT  word_count,  COUNT(DISTINCT word) AS distinct_word_count,  COUNT(DISTINCT corpus) AS distinct_corpus_count,  STRING_AGG(word, \"\u0026\" LIMIT 3) AS string_agg_word,  ARRAY_AGG(word LIMIT 2) AS array_agg_wordFROM  `bigquery-public-data.samples.shakespeare`GROUP BY  word_countORDER BY   word_count行word_countdistinct_count_worddistinct_count_corpusstring_agg_wordarray_agg_word113019842LVII\u0026augurs\u0026dimm'dLVIIaugurs22894642cheque'd\u0026affords\u0026meetcheque'daffords..................集計関数単体だと tips というほどの情報もないのだが、この (入力行):(出力行) = N:1 という関係を意識しつつ分析関数と併せて理解しておくと見通しがよくなることが多い。分析関数分析関数に関する公式リファレンスは https://cloud.google.com/bigquery/docs/reference/standard-sql/analytic-function-concepts である。分析関数は入力行と出力行が N:N になるもので、例えば以下のクエリは word_count が同じものを一つのグループにして、その中で word の長さが長い順に rank をつけるというクエリである。SELECT  word,  word_count,  RANK() OVER (PARTITION BY word_count ORDER BY LENGTH(word) DESC) AS rank_word_count_lengthFROM  `bigquery-public-data.samples.shakespeare`ORDER BY  word DESC行wordword_countrank_word_count_length1zwaggered184482zounds1466673zone180132............最も重要なのは OVER 句で、ここで行のグループがどの単位なるのかを指定している。今回は PARTITION BY word_count で word_count が同じもの毎にグループを作っている。いわゆる WINDOW というやつですね。この WINDOW はこのように指定することもできるが、以下のように named window を使って書くこともできるが、あまり使ってるのを見たことがないし自分も使わない（可読性がそんなによくないので）。コードブロックのコードシンタックスも効いてない。SELECT  word,  word_count,  RANK() OVER (word_count_window) AS rank_word_count_lengthFROM  `bigquery-public-data.samples.shakespeare`WINDOW  word_count_window AS (PARTITION BY word_count ORDER BY LENGTH(word) DESC)ORDER BY  word DESCもう一つポイントになるのは結果を返すのはグループにした各行それぞれということ。今回の例でいうと、例えば word_count が同じものが 100 行あったとしたら、各 100 行に対して結果を返している。ランクをつけるってそういうことなんだから当たり前だろ、というのはごもっともだが、これが分析関数と呼ばれるものだとちゃんと意識しておくのは役に立つ。例えば、各 word 毎に、その word の長さを全ての単語の中で最も長い単語で割った時の割合も一緒に結果に出すようなクエリを書きたいとしよう。これを集計関数・分析関数の区別なくとりあえずこんな感じか！？と書いてエラーに遭遇するという経験をした人がいるかもしれない。-- これはエラーになる: SELECT list expression references column word which is neither grouped nor aggregated at xxxSELECT  word,  word_count,  LENGTH(word) / MAX(LENGTH(word)) AS ratio_word_lengthFROM  `bigquery-public-data.samples.shakespeare`ORDER BY  ratio_word_length DESC, wordこの MAX は集計関数で、入力行と出力行が N:1 になるものなので、各行に結果を返すわけではないので word や word_cout が返す行数がマッチしてないのでエラーになる。SELECT list expression references column word which is neither grouped nor aggregated at xxx というエラーで、集計されてないので集計して行数がマッチするようにしなさいというものである。このエラーだけ見て集計するようなクエリを書かないといけないか〜と考えるのではなく、これは集計関数の N : 1 の関係なので問題が起こるのであって、N : N の分析関数に変換できればいいんだなと考えれば、余分なクエリを書かずに対処ができる。多くの集計関数は OVER 句を付与することで分析関数になるということを押さえておくと、以下のように書くことで目的が達成できる。SELECT  word,  word_count,  LENGTH(word) / MAX(LENGTH(word)) OVER () AS ratio_word_lengthFROM  `bigquery-public-data.samples.shakespeare`ORDER BY  ratio_word_length DESC, word行wordword_countratio_word_length1honorificabilitudinitatibus11.02Anthropophaginian10.62962962962962973indistinguishable10.6296296296296297............分析関数は分析クエリを書くときに多用するので、この辺りの振る舞いを押さえておくと、冗長で読みにくいクエリでなくてすっきりしたクエリを書けることが多い。ちなみに WINDOW の柔軟な指定方法に関してはここでは触れなかったが、自分はあの文法は全然覚えられないので、必要が生じたらリファレンスを読みながら書くようにしている。tips5: サブクエリ一口にサブクエリと言っても色々あるが、ここではテーブルサブクエリと式サブクエリについて書く。テーブルサブクエリはテーブルを指定する位置で () で括ったクエリを書くことで利用できる。これを使って先ほどと同じケースを書いてみると以下のようになる（分析関数を知っているとこれは全然よい書き方ではないと分かる）。SELECT  word,  word_count,  LENGTH(word) / max_length_word AS ratio_word_lengthFROM  `bigquery-public-data.samples.shakespeare`,  (SELECT MAX(LENGTH(word)) AS max_length_word FROM `bigquery-public-data.samples.shakespeare`)ORDER BY  ratio_word_length DESC, wordテーブルサブクエリは基本的には WITH 句で書く方が可読性が高いので WITH 句を使うのがいいが、例えば今回の例を WITH 句を使うと以下のように冗長な感じにもなるので、ちょっとしたクエリを書いて JOIN したりするときはテーブルサブクエリを使ったりすることがある（繰り返しだが、今回の例だと分析関数を使っておくのが最も良い選択肢とは思うということは前提として）。WITH  table_max_length_word AS (  SELECT    MAX(LENGTH(word)) AS max_length_word  FROM    `bigquery-public-data.samples.shakespeare`)SELECT  word,  word_count,  LENGTH(word) / max_length_word AS ratio_word_lengthFROM  `bigquery-public-data.samples.shakespeare`,  table_max_length_wordORDER BY  ratio_word_length DESC, word式サブクエリは式が使えるところで利用できるクエリで、こちらはいくつかの典型的なものがある。例えばスカラーサブクエリは式を指定する位置で () で括ったクエリを書くことで利用でき、例によってこれまでと同じものを表現すると以下のようになる。SELECT  word,  word_count,  LENGTH(word) / (SELECT MAX(LENGTH(word)) FROM `bigquery-public-data.samples.shakespeare`) AS ratio_word_lengthFROM  `bigquery-public-data.samples.shakespeare`ORDER BY  ratio_word_length DESC, wordスカラーサブクエリは単一行の結果を返さないとエラーになることは注意が必要で、特に相関スカラーサブクエリを書く時に条件を適切に設定しないといけない。例えば以下のようにクエリ（こんな書き方をする意味などないクエリだが、例として）を書くとエラーになり、これは word がユニークではないので WHERE 句の条件で単一行に絞れてないのでエラーとなる。これを適切に動かすようにするには、WHERE 句の条件で単一行になるように、例えば元のテーブルを DISTINCT word などして word がユニークになるようにする、などとする必要がある。-- これはエラーになる: Scalar subquery produced more than one elementSELECT  word,  word_count,  (SELECT LENGTH(word) FROM `bigquery-public-data.samples.shakespeare` WHERE t.word = word)FROM  `bigquery-public-data.samples.shakespeare` AS tスカラーサブクエリは他のテーブルの集計結果を持ってきて使用するとかには便利なのでちょっとした分析では使ったりするが、やりすぎると読みづらいので用法用量をお守りくださいという感じ。自分用の分析ではちょくちょく使うが、他人に共有するクエリの場合はあまり使わないように心がけている、くらいの温度感で使っている。式サブクエリの他のパターンとしては Array サブクエリがあり、名前の通り以下のような感じで Array を作るサブクエリを指す。SELECT  ARRAY(SELECT word FROM `bigquery-public-data.samples.shakespeare` LIMIT 5) AS array_test行array_test1LVIIaugursdimm'dplaguestreasonこれは返しているのは 1 行で、その 1 行が ARRAY 型で 5 個の要素を持っていることに注意。ARRAY は STRUCT と一緒に使われることも多いし、リテラルで書くときは SELECT [1,2,3] とか SELECT (1,2,3) のように同じような形で書いたりするので混同しがちだが、STRUCT() の方は以下のようにスカラー関数として機能しており、返してるのは 5 行である。SELECT  STRUCT(word, word_count) AS struct_testFROM  `bigquery-public-data.samples.shakespeare`LIMIT  5行struct_test.wordstruct_test.word_count1LVII12augurs13dimm'd14plagues15treason1つまり以下のように ARRAY\u003cSTRUCT\u003e 型を作ると、ARRAY 型として 1 行を返していて、その中の要素として 5 個の STRUCT が入ってるんだなと分かる。SELECT  ARRAY(SELECT STRUCT(word, word_count) FROM `bigquery-public-data.samples.shakespeare` LIMIT 5) AS array_struct_test行array_struct_test.wordarray_struct_test.word_count1LVII1augurs1dimm'd1plagues1treason1かなり基本的な話だが、こういう簡単なところで違いをちゃんと認識しておくと ARRAY と STRUCT を区別できて変なクエリを書いてエラーになることが少なくなる。続いて IN サブクエリで、以下のように WHERE 句で条件指定するときにサブクエリを使うところで出番が多い。SELECT  *FROM  `bigquery-public-data.samples.shakespeare`WHERE  word IN (SELECT DISTINCT title FROM `bigquery-public-data.samples.wikipedia`)ORDER BY  LENGTH(word) DESC行wordword_countcorpuscorpus_date1Northamptonshire1kingjohn15962Gloucestershire3kingrichardii15953Gloucestershire21kinghenryiv1597................これもちょっとした条件をささっと書くのに便利だったりする。IN の指定は IN (1,2,3) のように直接要素を入れることができたり、IN UNNEST([1,2,3]) のように ARRAY を UNNEST したもので書くことができたり、色んなバリエーションがある。あまり考えずに使っている分にはまあ便利な書き方かなという感じだが、真面目にここでの () はどういう意味なのか理解しようとしたり文法的に意味のあるものとして解釈しようとしてもうまくいかないので、こういうパターンもあるのね、というくらいに思っておくのが精神衛生上いいとは思う。tips6: 型変換型変換について簡単に書く。公式リファレンスは https://cloud.google.com/bigquery/docs/reference/standard-sql/conversion_rules である。明示的な変換である cast と暗黙的な変換である coerce (強制型変換) がある。前者は特に意識することもなく型変換を明示的にしたい場合に使えばいいが、後者はお手軽にクエリを書くときに（場合によっては知らず知らずのうちに）よくお世話になるもので、特に WHERE date_column = \"2021-01-01\" みたいな形で使うことが多い。以下のように、STRING を特別な形式で書くと様々な時間に関する型へと強制型変換してくれる。SELECT  TIME(00, 00, 00) = \"00:00:00\" AS test_time,  TIMESTAMP(\"2021-01-01 00:00:00\") = \"2021-01-01\" AS test_timestamp,  DATE(\"2021-01-01\") = \"2021-01-01\" AS test_date,  DATETIME(\"2021-01-01T00:00:0\") = \"2021-01-01\" AS test_datetime行test_timetest_timestamptest_datetest_datetime1truetruetruetrueこれはめちゃくちゃよく使うのでちゃんと意識しておくのがよい。型変換に関しては supertype も頭に入れておくのがよい。UNION ALL するときとか CASE 式を使うときに型が異なっていても supertype が同じなら処理できる。例えば以下は INT64 の supertype として FLOAT64 があるため、INT64 のデータが FLOAT64 に強制型変換されて FLOAT64 として扱われるようになる。SELECT 1 AS test_supertype UNION ALL SELECT 2.0行test_supertype11.022.020211121 現在、TIME、TIMESTAMP、DATE、DATETIME はそれ自身のみが supertype だと公式リファレンスには書いてあるが、実は DATE の supertype として DATETIME があることが以下のクエリが実行可能で結果が DATETIME になることから分かる。SELECT DATE(\"2021-01-01\") AS test_supertype UNION ALL SELECT DATETIME(\"2021-01-01T00:00:0\")行test_supertype12021-01-01T00:00:0022021-01-01T00:00:00BigQuery は型をよしなに型変換して扱ってくれるので便利な一方、型変換の基本的な振る舞いとか supertype がある程度頭に入ってないと気付かぬ間に期待と異なる型を取り扱っていたり、以前似たようなクエリ書いたときはエラーにならなかったのに今回はエラーになって困ったり、などが発生し得る。今回取り上げたことだけでも頭に入っていれば分析用のクエリを書く時ににそこそこ役に立つのではないかなと思う。まとめBigQuery で分析する際の part2 としてスカラー関数・集計関数・分析関数、サブクエリ、型変換について書いた。自分のブログはコードブロックの表示が綺麗ではないのでそろそろ改善したい気持ちになってきた…","link":"https://yoheikikuta.github.io/BigQuery_tips_part2/","isoDate":"2021-11-21T00:00:00.000Z","dateMiliSeconds":1637452800000,"authorName":"yoheikikuta","authorId":"yoheikikuta"},{"title":"BigQuery を使って分析する際の tips (part1)","contentSnippet":"TL;DRBigQuery で分析する際の tips をまとめてみる。長くなりそうなのでいくつかに分割して書くpart1 はエディタとして何を使うかとか実行結果の連携などについて書くBigQuery console/DataGrip を使いつつ、結果を GitHub issues/Google Sheets/Bdash Server で共有するという感じで使っている仕事で BigQuery を使って分析することが多いので、いくつかの回に分けて BigQuery を使って分析する際の tips をまとめていくことにする。今回は part1 としてエディタとして何を使うかとか実行結果の連携などについて書く。個人的な探索的・アドホック分析用途の話に限定して、組織的にどういうデータ分析基盤を使うかとかそういう話はしない（会社だと ETL の L として dbt https://www.getdbt.com/ を使っていて、これについても色々と書きたいことはあるけどそれはまた別の機会に）。tips1: クエリを書くためのエディタ基本的には BigQuery console https://console.cloud.google.com/bigquery を使ってきたし、今でも使っている。しかしながら、これは BigQuery console を使っている人全員が感じていることだと思うが、イマイチと言わざるを得ない。ブラウザで提供してるので補完が遅いのはやむなしと思う（この遅さによって書いてる時にちょくちょくスムースにいかなくてストレスが溜まることもあるが）けど、フォーマッターはとりあえずこれで納得するかというレベルにならない（CASE 式を見るたびに足が震えてしまう）し、ちょっと前に新 UI が出た時も「これは本当に開発者が触って使い心地チェックしてるのだろうか？」という感じだった（同僚の中には使いづらいのでエディタタブを無効にしているという人もいる）。BigQuery は間違いなく素晴らしいプロダクトではあるが、その「プロダクト」には UI の観点があまり含まれていないようで、こういうところは残念である。もちろんいいところもたくさんあって、ブラウザがあれば使えるというお手軽さは最高だし、実行前の検査でクエリスキャン量を表示したりや文法エラーに留まらずより広範囲のエラーを教えてくれたり（例えば異なる region にある dataset のテーブルを join は不可能だが、それをクエリ実行前に教えてくれる）、Google Sheets や GCS への export などがシームレスに実行できる点、など重宝している。個人的にはフォーマッターがマシになってくれてかつそのフォーマッターが公開されてくれれば、BigQuery console 一本でやっていくという選択肢を取ってもいいかなと思っているが、待ち続けてもそういう日がやって来る気配はない。そんなこんなで JetBrains 社の DataGrip https://www.jetbrains.com/datagrip/ も併用している。これはデータベース IDE で JetBrains 社の製品！という感じの rich な機能を提供してくれる。書き味はざっくりこんな感じ。FROM 句書く時に謎の空白行を入れてるけど、これはテーブル名の変換候補の表示で会社で使っている project/dataset/table が見えないようにするためです。JetBranins 社製品だけあって、補完の滑らかさはかなり心地良い。その他にも文を複数書いてもよしなに各文を解釈してくれるので CMD + Enter とカーソル選択のみで特定の文を選択的に実行できるとか、F1 で Quick Documentation してテーブルの概要を把握できるとか、Export Data でクエリ結果を markdown table にして Copy to Clickboard して GitHub issues に貼れるとか、便利な機能が多い。フォーマッター Editor \u003e Code Style \u003e SQL \u003e General で実際にどのようにフォーマットされるのかの例が以下のように視覚的に分かりやすい形で柔軟に色々と設定することができる（フォーマッターとかあれこれ細かく設定したくないのでこれ使っとけばだいたいオッケーというものをチームで使うようにしたいのだが…）。DataGrip は色々なデータソースに対応していて、BigQuery 対応は比較的最近（2020.2）始まったので、まだまだ不完全な部分もある。やはり Array型 とか STRUCT型 周りが弱い感じがする。補完が弱いのはまあそんなに気にならないが、手元で色々分析する時に例えば id だけ入れ替えれば使いまわせるような scripting statements https://cloud.google.com/bigquery/docs/reference/standard-sql/scripting を使ったりするので、これがサポートされてないのはちょっと不便だ。それと JetBrains 社製品ということでそこそこの金額がするのも気になるところ。会社で働いていてがっつり使うという場合は問題ないが、そんなに頻度高くなく必要になったらちょっと使いたいくらいの場合はなかなか購入まで踏み切りづらいかもしれない。local でのクエリの保存とちょっとしたグラフを見るという用途で Bdash https://github.com/bdash-app/bdash も使っている。local での保存という意味では別に Bdash を使わなくてもいいのだが、後述するように Bdash Server https://github.com/bdash-app/bdash-server で他の人にクエリを共有するというのにも便利なので Bdash を使っている。作者が社内にいるのでなんかサポートされてない機能があったら feature request を出せるのも便利（contribute しろよという意見は一旦無視する）。tips2: Colaboratory での利用クエリだけでなく、Python で例えば pandas.DataFrame に結果を読み込んで interactive に色々分析したいということもある。ちょっとした分析が目的のときには local に環境準備をするのは面倒だし、他の人に再現可能な形で共有するのにも適してない。そういうときにはやはり Colaboratory https://colab.research.google.com/notebooks/welcome.ipynb を使うのは便利である。BigQuery にクエリを投げて結果を利用する方法はいくつかあるが、一番お手軽なのは magic commands を使うことだろう。自分は以下のコードを snippets に登録して使えるようにしている。from google.colab import authauth.authenticate_user()%%bigquery --project [適当な GCP project] dfselect * from ``これで interactive に authentification をして、クエリ実行結果を df という pandas.DataFrame に格納することができる。Colaboratory は URL を共有してコピーして使ってもらえば他の人も（自分と同じような環境構築をしてもらうとかの手間がなく）簡単に利用できる。ちなみに自分がいま分析してる範囲だとそんなに Python を使ってあれこれ分析せずに SQL だけで十分なことも多いので、そんなに使ってはいない。ちょっと話は逸れるが、分析やモデリング周りを一手に担う統合プラットフォームとして Vertex AI https://cloud.google.com/vertex-ai が実際どんな感じかはやや気になっている。ここまで色々揃っていると、このプラットフォームに沿うように自分たちがうまく振る舞わないといけないと思うが、そうしたときにどれくらいのメリットを享受できるのかというのはガッツリ運用している人がいればぜひ聞いてみたいところだ。tips3: 実行結果の連携方法分析結果をどう連携するかについても日々実践していることを書いてみる。一番頻度高く使うのは、クエリの実行結果を作業ログとしての GitHub issues に markdown table として貼るというもの。これは連携というよりは自分の日々の作業ログであったり他の人が作業ログを見た時に自分の思考の流れが追えるようにしてるという側面が強い（ちなみに以前データ分析系タスクの作業ログ共有に Jasper を使っているという話 https://yoheikikuta.github.io/working_log_using_jasper/ も書いたりしている）。markdown table としてコピーする方法は BigQuery なら以下のように tweet した方法を使っていて、DataGrip の場合は Export Data で Copy to Clickboard を使っている（当然後者の方が使いやすい）。BQ console のクエリ結果をコピーして GitHub issues とかにマークダウンとしてペーストできるのは便利だけど、左だとヘッダーがちゃんととれなくて右だと大丈夫（これは Safari で Chrome だと見た目の違いは分からない）tips は左上からではなく右下の値のところから範囲選択することです（真顔） pic.twitter.com/Th3h99C5Qt— Yohei KIKUTA (@yohei_kikuta) November 7, 2021 次によく使うのは Google Sheets に結果を export して連携するというもの。Google Sheets はソフトウェアエンジニア以外でも使える人が多いので、結果を共有したり interactive にちょっと触ってもらうのには適している。BigQuery console だと 結果の保存 \u003e Google スプレッドシート でスッと export できるし、DataGrip ならば Export Data で TSV にして Copy to Clickboard でコピーしてから Google Sheets にペーストしている。ちょっとダルいのは、特に前者の方法だと個人の Google Drive に保存されてそのままでは他の人が使えないので、共有できるよう保存場所を移動するなりしないといけないという点。Google Sheets という意味では Connected Sheets https://cloud.google.com/bigquery/docs/connected-sheets を使うのも便利。単純にクエリの結果を貼って共有するのとは違い、こちらはちょっとしたクエリを書いてその実行結果を Google Sheets 上で扱えるというのが利点で、さらに定期実行を設定できるので新しいデータを参照してもらうことができる。自分はちょっとした分析結果を使ってビジネス的な意思決定のインプットにしてもらう、とかいうときに日時で新しい結果を参照できるような Connected Sheets を共有したりする。もちろんこの Connected Sheets 上で複雑なクエリを書いたり、あれもこれもとやりすぎると管理や運用が大変になってくるので、よく使うし重要というものはちゃんとデータマートを準備したり BI ツールで閲覧できるようにしたりなどと交通整理する必要はある。Data Portal とか BI tool での連携というのはアドホックというよりもう少しかっちり運用という感じになるので、今回は対象外としておく。連携というほどにはまだ会社全体で使い倒してるレベルにはないが、Bdash Server が導入されてるのでクエリと結果を共有（というか他の人も見れたら便利かもなというものを見れるとこに置いておくだけという感じ）するときに使っている。Bdash からワンクリックで以下のように共有できるので、local での自分用のクエリ保存と簡単なグラフ確認を Bdash でして、それを他の人も共有できるように Bdash Server にも送っておくという使い方をしている。まとめBigQuery で分析する際の part1 としてエディタとして何を使うかとか実行結果の連携などについて書いた。","link":"https://yoheikikuta.github.io/BigQuery_tips_part1/","isoDate":"2021-11-13T00:00:00.000Z","dateMiliSeconds":1636761600000,"authorName":"yoheikikuta","authorId":"yoheikikuta"},{"title":"Launchable の人とカジュアル面談した","contentSnippet":"TL;DR興味があったので Launchable の @yoshiori さん @draftcode さんとカジュアル面談をしたテストをはじめとしてソフトウェアエンジニアリングの生産性をデータドリブンで改善していくというのはめちゃ面白そう自分は現段階では転職意思がないけど、機械学習エンジニアで興味ある人は言ってくれればおつなぎします！Ubie Discovery に転職して一年半が過ぎた。そんな折に Launchable https://www.launchableinc.com/ で Machine Learning Engineer の position を募集しているというを見て、yoshiroi さんが転職した会社だし面白そうなので話を聞いてみたいなと思って連絡したら、転職の意思があまりないと事前に伝えていたにも関わらず快諾してもらった。現時点では自分が転職するということはないが、興味ある人もいるだろうということでブログに残しておく。（話の内容をブログにしてもよいというのはすでに先方に確認を取っています）経緯書いた通りだが、思い立って連絡したら一時間ほど話をしてもらえることになった。最近入社した @draftcode さんも呼んでもらって三人で話をすることになった。自分としても面白そうだと思っていた分野なので、話の前に HP を見て紹介されていた https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45861.pdf や https://engineering.fb.com/2018/11/21/developer-tools/predictive-test-selection/ やちょっとググって関連知識を調べるくらいはしておいた。Launchable の会社全般的な話会社の目指すところはデータドリブンでソフトウェアエンジニアリングの生産性を上げるというもので、その最初の取り組みとして特にテストに注力しているとのことらしい。ソフトウェアエンジニアリングはどのように不確実性と向き合うかというのを積み上げてきてはいるが、その中には確率的に判断せざるを得ないものもあるし、そうなると適切にデータを集めて統計的に考え行動する必要があると思う。そういったところに真正面から取り組んでいる Launchable の目的には共感する。ビジネスモデルは toB 向けのフリーミアムモデルで、HP の pricing を見るとなかなか面白くて、$7 / hours saved ということで開発が効率的になった分だけお金を支払うというものになっているらしい。マーケットはソフトウェア開発をしている企業全般ということになるのでポテンシャルはとても大きい。とはいえ会社ごとに違いはありますよね？という疑問が思い浮かぶので聞いてみたが、会社ごとにセットアップしてサービスを展開してきて、色々な勘所が分かってきたという段階らしい。カスタマイズを要求されまくって破綻するみたいなことはなさそうな一方で、もっとスケールさせるためにはまだまだアイデアやチャレンジが必要そう、みたいな印象。最近優秀なソフトウェアエンジニアの人が続々入社してる会社なので、一緒に働く同僚から得られる刺激が多そうなのもよさそう。Launchable の機械学習周りの話機械学習周りの話も色々と聞いてみた。ファイルの変更行数などを入力特徴量として、各テストの fail する確率を予測する GBDT を使うというのが基本になっているらしい。テスト実行を積み重ねることによってそこから単なる 成功/失敗 以上の情報を抽出するという試みは興味深い方向性だ。同じ会社でもレポジトリによって触る人が全然違うし、テストは複数レポジトリにまたがることもあるので、モデルを学習する単位としては一連のテストを実行するプロジェクト単位で作っているとのこと。ソフトウェアエンジニアリングに造詣がありつつも軸足は機械学習にある、というタイプの人がマッチしそう。ということで、Launchable は機械学習エンジニア絶賛募集中とのことです！ソフトウェアエンジニアは採用が順調だけど、機械学習エンジニアはなかなか path がないとのことだったので、私からも宣伝しておきます。もちろん一番は Ubie Discovery に興味を持って話を聞かせてくださいというのを期待しますが、Ubie Discovery よりも Launchable の方が興味があるんや！という場合も遠慮なく言ってください。ちなみに拠点は JP と US だけど、開発拠点は日本が中心とのことです。菊田さんぜひ！というありがたいお言葉もいただきましたが、自分の興味関心から考えるとやはり Ubie だなという気持なので、何か状況が変わるタイミングがあったりしたらまた話をさせていただきたい、という感じでカジュアル面談は着地しました。まとめ興味があったので Launchable の話を聞いてみたら面白そうだったのでブログに書いてみた。ちなみにあわよくば話をした二人とも Ubie Discovery に引っ張ってこようとして話の最中にどんどんこちらの話もねじ込んだのですが、さすがに転職するという雰囲気はありませんでした。ベンチャー企業同士仲良くやっていきましょう！","link":"https://yoheikikuta.github.io/casual_talk_launchable/","isoDate":"2021-10-26T00:00:00.000Z","dateMiliSeconds":1635206400000,"authorName":"yoheikikuta","authorId":"yoheikikuta"},{"title":"施策デザインのための機械学習入門 を読んだ","contentSnippet":"TL;DR機械学習に基づく施策をサービス改善につなげるためのフレームワークとその実践を提供してくれる本統一的なフレームワークと一貫した記述で、よく書かれている本だなと感心した個人的には 4.2 節の Implicit Feedbak を用いたバイアスを考慮したランキングシステムの構築のところは勉強になったタイトルの通り 施策デザインのための機械学習入門 という本を読んだ。本の正式なタイトルは「施策デザインのための機械学習入門　データ分析技術のビジネス活用における正しい考え方」というもの。本のタイトルからは具体的ににどういう内容が書いてるかは想像しにくいが、内容は機械学習に基づく施策を実際に導入する際に重要となる観点を一連のフレームワークとして提供し、そのフレームワークに基づいた具体的な実践を解説するというものになっている。技術評論社出版で HP は https://gihyo.jp/book/2021/978-4-297-12224-9 にある。https://github.com/ghmagazine/ml_design_book にある。本の概要章立ては以下のようになっている。1 章 機械学習実践のためのフレームワーク    KPI 設定 → データの観測構造をモデル化する → 解くべき問題を特定する → 観測データを用いて解くべき問題を近似する → 機械学習モデルを学習する → 施策を導入する というフレームワークを導入2 章 機械学習実践のための基礎技術    広告配信を例に導入したフレームワークに則って施策導入までの一連の流れを解説3 章 Explicit Feedbackに基づく推薦システムの構築    ユーザの行動に基づくデータの観測構造に注意を払って適切な目的関数を導く一連の流れと実験結果を解説4 章 Implicit Feedbackに基づくランキングシステムの構築    ポジションバイアス、セレクションバイアス、クリックノイズ、に対して適切な目的関数を導く一連の流れと実験結果を解説5 章 因果効果を考慮したランキングシステムの構築    より発展的な内容として推薦枠非経由の conversion も考慮したケースを紹介演習問題    本の内容を自分で実践に活かすための訓練となるような問題大まかな頭出しについてはこれより詳しいものとして著者のブログ https://usaito.hatenablog.com/entry/2021/08/03/191339 があるので、もう少し詳しく知りたい人はこれを読んでから本を買うかどうか決めればよさそう。全体的によく書けていて、入門書を謳って基本的なところから始まりつつも内容が割としっかりしていて、自分にとっても勉強になるところが多い本だった。推薦システムは 5 年くらい前に仕事でやって以来そんなに追ってなくてほとんど覚えてないこともあり、特に 4.2 節の Implicit Feedbak を用いたバイアスを考慮したランキングシステムの構築のところとかは勉強になった。数学的な定式化がしっかり目になされているので読みやすい。特によく書けているなと感じたところこのエントリではそれぞれの章でどういうことを書いてあるかを紹介したりはせず、自分が特によく書けているなと感じたところを書く。全編を通して一貫したフレームワークに基づく記述1 章で導入する機械学習実践のためのフレームワークに基づいた一貫した記述がなされている点がよい。本を書こうとすると、どうしても色々な知識を紹介しようとしてあれこれと詰め込み、結果として知識の羅列を薄く繋いだものになりがちだと思う。この本はそうではなくて、様々な具体的ケースに対応できる適度なレベルに抽象化したフレームワークを構築し、それで一本筋が通っている状態で個々の具体例を解説している。具体例で経験を積み、このフレームワークに立ち戻って理解を増強することで、より困難な他の具体例に立ち向かうことができるようになるという設計になっている。こういう思想は巻末の演習問題にも顕著に現れている。この統一的な記述は著者のこだわりというか信念が感じられてとてもよい。データの観測構造のモデル化と unbiased な目的関数の導出手順フレームワークの中でも特によいと感じたのがデータの観測構造のモデル化と 解くべき問題を近似するときの unbiased な目的関数を導出するところである。データの観測構造のモデル化のところは解析可能な数学的表現で対象の状況をモデル化するという意味合いが強い。この部分がしっかり目に書かれているのがよいところで、ここがきちんとモデル化できないと解くべき問題を数学的に表現できず、バイアスなどの落とし穴に気づくこともできない。（数学的表現に慣れてない人がこの本で機械学習に入門するぞ、という場合には少し難しいかもしれない）。データの観測構造をモデル化できるからこそ、解くべき問題を近似して適切な目的関数を設定するという後のプロセスも適切に実施できる。この本ではまずは観測データのみに注目してデータの観測構造を意識していないナイーブ推定量を算出し、それだとデータの観測構造に基づくバイアスが乗っているのでそれを是正するように Inverse Propensity Score 推定量（バイアスを打ち消すような重みを逆数として掛ける）を使いましょうという流れになっている。これを何度も繰り返すので、どうやって適切に解くべき問題を近似するのかというのが意識しやすくなっている。手計算できる簡単な例で感覚を掴む問題の本質を損なわないようにしつつ手計算で確認できる例が豊富にあるのもよい。ある種のアイデアがあるときに、それを正しく把握するためには（本質を損なわない）簡単な例でチェックするというのは重要で、そこを意識して簡単化した例がいくつも載せられている。簡単な例で振る舞いを理解する、次元解析をする、極端なケースで振る舞いを確認する、まずは生データを眺める（EDA する）、などは感覚を掴むために重要なので、実データで取り組む前に簡単だが重要な例があるのはよいと思った。余談だが、こういう話を書くと「いかにして問題をとくか」が思い出されるわけだが、これの機械学習/データ分析特化版みたいなのがあっても面白いかもしれない（断片的には色々なところで語られていると思うけど）。実験用データを問題設定に合わせて適宜いじっているオープンデータを使って実験する際に、本の内容を解説するために適宜手を加えてデータセット（本では半人工データと呼んでいる）を準備しているのも工夫が感じられた。機械学習の本を読んでると、それでは具体例と言ってこのデータにこのモデルを適用したら結果はこう！みたいな感じでそこから何かを得るということがあまりないことも多いが、本の内容を適切にサポートするような実験設定にしてあるところは素敵だなと思った。その他記述が丁寧すぎる部分がある。入門書ということなので丁寧にしたのだと思うけど、unbiased な目的変数の期待値が望み通りになっているかの計算がほぼ自明なのに繰り返し記述されてるのとかは、本書を通読できる人なら必要ないと感じた。もう少し記述をスキップした上で、章末の発展的内容を一部解説してみたり 5 章の内容で著者が考えてることをもっと深く記述してもらうとかだと個人的には嬉しかった。notation がちょっと confusing な部分がある。4.2 節の Pair Result Randomization のところで確率的ランキングを導入するが、ユーザ \\(j\\) に提示するランキング \\(y_j\\) にさらに \\(k \\leftrightarrow k+1\\) を swap するランキングと元のランキングという新しい次元の情報が出てくる。それらを断りなく \\(y_1, y_2\\) と書いたりするのでユーザの添字と confusing なところがある。まあこれはさらに新しい notation を導入したりすると複雑になりすぎるし分かるやろってことでそう書いてるのだと感じたけど、ちょっと読み淀んだ。あと \\(y^{-1}\\) でアイテムを指すのはあまりしっくりこなかったけど、推薦システム関連だとよく使われる notation なのかな。この本に限らずだけど、typo とか見つけたらフィードバックしたいけど、どこで受け付けてるか書いてくれると嬉しい。正誤表 https://gihyo.jp/assets/files/book/2021/978-4-297-12224-9/download/%E6%AD%A3%E8%AA%A4%E8%A1%A82021-08-18.pdf にないところだと図 4.5 の \\(R(u,i_9) = 0\\) でなく \\(R(u,i_9) = 1\\) というのがあった（本文を読めば分かるので単なる typo）。GitHub repository はコードだけって感じだったので書いていいか判断つかず、技術評論社のお問い合わせページ https://gihyo.jp/site/inquiry/book?ISBN=978-4-297-12224-9 に送っておいた。ただこちらは他の人が何送ってるかが見えないので被ったりしそう。GitHub repository あるなら issues にそういうフィードバックどうぞと案内するとよさそうか。まとめ施策デザインのための機械学習入門 を読んだ。よい本だったので書評ブログを書いたが、改めてブログとして書いてみると、著者が多大な努力を払って本を完成させたことが伺え、ありがとうございますという気持ち。","link":"https://yoheikikuta.github.io/ml_design_book/","isoDate":"2021-10-01T00:00:00.000Z","dateMiliSeconds":1633046400000,"authorName":"yoheikikuta","authorId":"yoheikikuta"},{"title":"自分がデータ分析/機械学習で成し遂げたいこと","contentSnippet":"TL;DRUbie 株式会社に入社して一年くらい経ったので振り返りをする改めてデータ分析/機械学習で何がしたいのかを考えているが、自分はデータから普遍的な知識を得たい人間で、その観点では（まだまだ先は長いけど）やりたいことができている人によって目的は色々だと思ういますが、みなさんはどうですか？Ubie 株式会社に入社して 1 年以上経過したので軽く振り返りをしつつ、そもそも自分はデータ分析/機械学習で何がしたいんだっけというのを改めて言語化してみるエントリ。ここしばらくは採用の文脈以外で他の会社の人と話すことが少なくて物足りないところもあるので、自分はこうだな〜という意見があればぜひ教えてください。一年間を振り返ってみるまず何よりも、勤続一年一ヶ月を達成している自分を褒めたい。凄いぞ自分。偉いぞ自分。勤続一年一ヶ月という圧倒的事実が示唆しているように、（労働の辛さはもちろんあるのだが）楽しく働けている。これは主観的にもそうだし、他の人から言われることもあるので客観的にもそうだと言えるだろう。入社を決めた時のブログエントリ https://yoheikikuta.github.io/joinning_ubie/ を今一度読んでみると、「ストーリーを語りたくなるような仕事ができそう」ということで入社を決めている。どうですか、ストーリー、語れそうですか？（自分への問いかけ）これに関しては力強く YES という回答なのだが、まだまだ「俺たちの戦いはこれからだ！」という状況なので、先を見据えつつも今やるべきことを着実に進めている。入社してしばらくは地道なバグの調査・修正とかデータの確認とかでしんどいタスクも結構あったのだが、そういうところをある程度乗り越え、今は有用なデータを蓄積することにフォーカスしつつ同時にそれを活用するというフェーズになっている。有用なデータを蓄積することは楽しい。簡単化した例として P(頭痛|髄膜炎) という条件付き確率を求めることを考えてみる。これは髄膜炎という病気に罹っているときに頭痛という症状を発症する確率を算出することを意味している。医学的な知識がない人にはこの一つの例であっても想像も難しいレベルだろう。医師であれば髄膜炎において頭痛は典型的な症状であって高い確率で発現することを知っているが、定量的に表現するということは簡単ではない（一人の医師が見聞きできる症例は限られているし、膨大にある症状と疾患の組み合わせを定量的に表現することは困難である）。十分な量のデータがあれば、髄膜炎に罹っている患者全体のデータから頭痛症状有りの患者の割合を求めることでこの条件付き確率を求めることができる。シンプルだが、シンプルであるが故に、データを集めるだけで医師の専門知識を誰にでも利用できる普遍的な知識へと昇華することができる。実際には、そもそものデータの真正性とか、年齢や性別による差異とか、既往歴や他の症状との関係性とか、気にすべき観点やより現実に即した発展は山のようにある。まだまだ先は長いが、こういったところを自分たちでデザインして進めていくのは実に楽しい。入社してから気付いた良さとして、一緒に働く機会がなかなかない業種である医師と同じチームで働けるという点が挙げられる。自分がほとんど触れてこなかった知識体系について造詣が深い専門家と一緒に働くと新鮮な驚きがあって楽しい。体調が悪い時に専門家に相談できるというのも優れた福利厚生と言える。福利厚生と言ってるが、社内医師が善意で相談に乗ってくれているという話で、これは仕事とは直接関係ないプログラミングの問題についてプログラマが答えてくれるようなもの。自分は胃痛でピロリ菌感染疑いだったとき、とりあえず自分で近場のクリニックに行ったら治療方針がよく分からずイマイチだったが、社内医師に相談しつつクリニックを変えたらあっさりと治療ができた。医師の専門性などを把握して適切な医療機関にかかるというのは簡単ではないなと実感したので、テクノロジーで人々を適切な医療に案内する、やっていきたいね。その他にも組織全体に関する業務や採用なども頑張っているが、これは同僚がたくさん情報発信してるので割愛。いいことばかりを書いたが、満足してないこともある。自分はデータ分析や機械学習の技術的に進んだ領域にも興味があるが、そういうところはこれまで殆どできていない。仕事とは別で摂取するしかないなと思って hikifune.fm を始めて補完していた。ただ、データが集まり人も集まり、この点に関しても色々なことに挑戦できそうな土台ができてきたので、今後は業務でもやっていけそう！自分はデータ分析/機械学習で何を成し遂げたいのかやってきた仕事を振り返りつつ、改めてデータ分析/機械学習を通じて何がしたいのかと考えてみると、自分はデータから普遍的な知識を抽出したいという嗜好性が高い。働き始めのころは今よりも数理的なモデリングへの興味が強く、現実のデータでモデルを作ってそこから意味のある情報を得ることに注力していた。仕事としてそれを実現するために、モデリング業務を中心に担当してサービスを改善・開発し、一部の内容は論文化したりと頑張っていた。こういう仕事はやっているとき楽しいと思っていたし、仕事始めのほぼ何も知らなかったところから考えると、相当に色々なことを学ぶこともできた。しかしながら、この手の仕事を続けていくうちに、自分は本当に何か普遍的な知識を得たのだろうか？という疑問を抱くようになってきた。発展的なモデルを駆使することでインパクトの大きな成果を出したり新しいサービスが作れたりする可能性は広がるけど、自分の人生の第一義的な目的はそういうものではないと知った。やはり入力となるデータに普遍的な知識を抽出できるポテンシャルがある領域が望ましい。そういった知識を積み上げることで、人類の知的財産に少しでも貢献できたな！という自己満足が得たいのである。一方で、可能なら労働なんてしたくないので金銭的にも十分なリターン（うまくいけば 5~10 年後には賃金のための労働をしなくてもいい）も欲しい。自分はビジネスで大成功するような才覚はないし、ライフプラン的にも給与所得だけでは難しさもあるので、期待値でいえば成長可能性が高い会社で Stock Option をもらうのが一番よさそう。こういった諸々の条件を満足し得る会社がどれくらいあるのか知らないが、自分が知る限り最も良い会社が Ubie 株式会社だった（偉そうに言ってるが @masa_kazama に声をかけてもらったおかげで知った。感謝）。前述の通り医療データには自分が望むポテンシャルが大いにあるし、会社として大きく成長する可能性も秘めている。当然どちらもうまくいくかの不確実性はまだまだ高いわけだけど、自分たちの力でそれを現実のものにしていこうと一丸となってやっていけるのは楽しい。もっと頑張っていきたいですね！たまにはこうやって「自分はそもそも何がやりたいんだっけ」を振り返ってみるのも一興ですね。ということで、みなさんはどうですか？ここまで自分の話をしてきたが、これは徹頭徹尾自分の好みであって、何が偉いとかどれが正しいとかこうあるべきとかいうものでは決してない。何を人生の目的関数とするかは人それぞれなので。ただし、目的関数に応じて適切な場所で働いた方がよいし、目的関数は時間と共に変わり得るものなのでその変化は認めて追従していった方がよい。例えば、機械学習を用いることで初めて実現可能となるようなサービスを作りたいと思っている人がいるとする。その人が日々の仕事ではまず機械学習を使わなくても済む方法を考えているとか、機械学習と関係のないコーディングがメインになっているとかいう状況であれば、目的とマッチしていないので環境を変えた方がいいだろう。データ分析や機械学習を活かしてサービスを改善したりビジネス的に大きな成果を出したいという人は、データの規模とかそれを効果的に使えるようなビジネスをしている会社に所属した方がいいだろう。このタイプの人は、入りはデータ分析や機械学習であってもやっていくうちに手法にはそんなに拘りがなくなって何でもやる人になるという印象がある。大学や研究所だけでなく企業でも研究職がそこそこあるので、研究を生業としてやっていきたいという人は他分野よりはやりやすそうだ。ただ、機械学習分野は参加人数が多くてカンファレンスに論文を通すために考慮しなければならない点も多いので、職業研究者がどうやって自分が本当にやりたい研究に取り組めるよう工夫をしているかは聞いてみたいところ。Kaggle のようなコンペ形式のデータ分析/機械学習が楽しいからそれに打ち込む、というのは目的が明瞭だし継続性もあってよさそう。多くの人が参加してその副次的な価値（コンペで得られた知見は直接関係のない仕事にも有用）を示してきた結果、仕事としても取り組めるような環境も出てきたのはいい話だな〜と思う。色々な目的関数があると思うので、どういう目的関数でそれを実現するためにいまどこで何をしてるか、というのはぜひ聞いてみたいですね。一年以上にも渡り他の会社の人と話す機会がなかなかなくて寂しいので、自分はこうだというのがある人は教えて欲しい〜。自分と似た考えの人で、Ubie 株式会社に興味がある人はお気軽にお声がけください。自分と似た考えじゃなくてもオッケーです。つまり興味があれば誰でも！！！まとめ仕事を振り返りつつ自分がデータ分析/機械学習で何を成し遂げたいのかを言語化してみた。自分はデータから普遍的な知識を得たいと思って今の会社で働いているけど、みなさんはどうですか？","link":"https://yoheikikuta.github.io/my_purpose_of_DS_and_ML/","isoDate":"2021-05-19T00:00:00.000Z","dateMiliSeconds":1621382400000,"authorName":"yoheikikuta","authorId":"yoheikikuta"},{"title":"hikifunefm 反省会会場","contentSnippet":"TL;DR15 回ほど配信をしたのでここらで hikifune.fm の振り返りをしてみる配信ドリブンで色々勉強したのでそこそこためになったし、シリーズ物はやってみて結構よかった二人で収録するメリットはあまり感じられなかったので今後どうするかは考え中hikifune.fm https://anchor.fm/yoheikikuta という podcast を始めて半年以上経ったので備忘録のために振り返りをしておく。反省会会場、と言いながらブログなので自分一人が振り返って書くだけなのだが、これはお察しの通りこのスレタイが好きだからつけただけである。hikifune.fm とは何だったのか？何だったのか？とか言うともう終わったものという感じもするが、まだ終わってはいない。コンセプトとか運用に関しては https://yoheikikuta.github.io/podcast_hikifunefm/ で書いた。機械学習に関連する事柄を配信することで自分たちの勉強に役立てようというものである（会社の仕事は諸般の事情により対外的に話しづらいということもあるので、勝手に勉強して勝手に話す場として準備している）。配信をしているので誰でも聞けるものではあるけど、聞き手にとってためになるとかではなくあくまで自分たちの役に立つという目的がメインで、一定間隔で配信をすることで配信ドリブンでインプットの意識を高めようという感じ。15 回配信をしてみての stats20210504 の時点で、総再生回数が 3,572 回で各エピソードの再生回数一覧は以下（正確には第 0 回もあったので 16 エピソードなのだが、第 0 回は単に始めます宣言なので一覧からは除いている）。一番再生されているのは Vision Transformer の論文解説で、一番再生されてないのは Prompt Tuning の論文解説（再生回数は時間に関して単調増加なので、最近の回が一番再生回数が少ないのはそれはそう）。内容的にある程度機械学習への理解がないと聞いても全く面白くないと思う Podcast なので、始める前は平均 100 回でも再生されれば結構凄いんじゃないかと考えていたので、思ったより再生回数は多いなと感じている。正直なところこの手の技術系 Podcast でどれくらい再生されるものなのか全く知見がないし調べてもいないので相場が分かってないが、hikifune.fm の場合はこんなもんだったということで。何か感想とか物申したいときのために Twitter hashtag #hikifunefm を準備したが、これは発表側以外では @karino2012 氏が感想を書いてくれたのみで他は殆どなかった。まあ自分も人の Podcast 聞いて感想を Twitter に書いたりしないので、そんなもんなんでしょうね。有名どころの Podcast だと感想を書いてる人もそこそこいる印象だけど、こっちはなんてたって曳舟だからね。ネームバリューがないよ（自分はもちろん曳舟好きだけどね）。show notes を GitHub repository https://github.com/yoheikikuta/hikifune.fm で管理していて誰でも issue に要望とか書けるようにしたけど、これは数としてはゼロ。Twitter hashtag 使われないくらいなんだから、そりゃ issue に何か書く人はいないでしょうな。この repository は issue を使うというよりは show notes をお手軽に管理したいという理由だったので、その目的は達した。良かった取り組み概ね隔週で配信してたけど、間隔としてはこれくらいがちょうどよかった感じがする。自分は発表者と聞き手を交互にやっていく役なので発表するのは 4 週に 1 回とかになる（実際はもうちょいハイペースだったが）ので、準備が怠くなってグダグダになりすぎるということはなかった。シリーズ物として DALL•E の理解に向けて というのをやったのは結構よかった。自分はあまりこういう方向性でやっていきたいというモチベーションがないので、単発で何かを話すだけだとあれこれ話題が飛ぶのであまり蓄積がなくて表面だけなぞって終わりがちだけど、3 回関連する話をすると自分の中でも理解が結構深まる。きっちりシリーズ物にしなくても、連続した数回はこういう系統の話でまとめる、とかやるとある程度はまとまった知識が得られるのでいいかもと思うなどする。show notes を割とちゃんと残した点もよかった。収録した音声はバグってないかは調べるけど最近はちゃんと聞き直したりはほぼしないので、この回はどんな話をしたっけというのを振り返りたい時に便利だったりした。そして収録前に repository に PR として送っておいて聞き手も見れるようにしておいたので、それを眺めながら聞くのはやりやすい（音声だけだとさっき何て言ってたっけ？とか忘れがちだけどテキストで書いてあるとすぐ確認できるので。Podcast 配信してるくせに身も蓋もない話だが、ちょっと込み入った話を音声だけで理解するというのは難しい）。イマイチだった取り組み二人で収録して聞き手を準備する、というのはそんなに効果がなかったなと思う。こういうのは大体準備ができるのが前日夜か当日収録前なので、聞き手は事前準備なく聞くことがほとんどで、質問がどうしても表面的で浅いものが多かった。まったく準備せずに聞き手として参加して素朴に質問することでリスナー目線で質問ができるかなという試みをしたりもしたが、そもそも自分たちの勉強のためにやってるものなのでやっぱりもうちょっと深い話に立ち入りたいよなというのが個人的な感想（それはそもそも Podcast で配信するような類のものではないのでは？という尤もらしい疑問は置いておいて）。その他にも細々した点はあるっちゃあるが、まあでも概ね良かったのではないか。ポジティブシンキングである。今後の方向性これを決めあぐねている。自分がやりたいと言って始めたもので他メンバーにちょっと負担を強いていた面もあったので、やっぱり発表者が自分以外には集まりづらい。他の人とワイワイやっていくスタイルにしたいなと最初は思ってたけど、自分一人でやっていくスタイルにするか。この場合は「あれ？なんで Podcast で配信してるんだっけ？ブログとか paper-reading とかでいいのでは？」となりそうな気配はする。発表内容を予め通達して、その内容に関して自分と同等以上に詳しい人を聞き手として招いて、自分の話に色々突っ込んだり補足を入れてもらうというのは自分の勉強にはよさそうだ。この場合は聞き手を探すのが大変という未来が容易に想像できるが。もともと自分たちの勉強になるというモチベーションで始めてある程度は役に立ったけど、そういうのはやっぱり少人数の勉強会で議論するのが一番いいんだよな（なかなか気軽にそれができる情勢になってはくれないのだが）。目的を変えて自分が興味ある話を紹介して自己満足する場にするか、ある程度やって満足したからもういいかとスパッと止めるか。どうするかまだ決めてないけど、ちょっと考えてみて近いうちに決めることにしよう。まとめhikifune.fm の反省会をした。今後どうしていくかはまだ未定。","link":"https://yoheikikuta.github.io/lookback_on_hikifunefm/","isoDate":"2021-05-02T00:00:00.000Z","dateMiliSeconds":1619913600000,"authorName":"yoheikikuta","authorId":"yoheikikuta"},{"title":"clasp と TypeScript で GAS の開発環境を整えた備忘録","contentSnippet":"TL;DRちょっとした仕事で GAS を使うときがあるローカルで clasp と TypeScript を使って開発する環境を整えた（が、いまの自分の用途ではここまでしなくてもよかった）完全に忘れそうなので備忘録としてブログに残しておく仕事で Google Workspace を使っていると、そのサービスを使ったちょっとしたタスクをするときに Google App Script (GAS) を使う時がある。スクリプトエディタで書いてたりもしたが、Git/GitHub での管理がしづらかったり、ローカルでの開発環境を提供する clasp https://github.com/google/clasp が TypeScript をサポートしているという話を知ったり（だいぶ遅い）して、せっかくなのでローカルでの開発環境を整えてみるかと思い立ってやってみた。一瞬で全て忘れそうなので備忘録としてブログに残しておく。諸々のインストール自分が使ってる Node と npm のバージョンは以下。$ node -vv15.9.0$ npm -v7.5.3npm で必要なパッケージをインストールする。TypeScript とか TSLint とかも入ってるけどそこはまあよしなに。$ npm install -g typescript tslint @google/clasp @types/google-apps-script自分が使っているのは以下のバージョンになっている。$ npm list -g/Users/yoheikikuta/.npm-global/lib├── @google/clasp@2.3.0├── @types/google-apps-script@1.0.25├── clasp@1.0.0├── tslint@6.1.3└── typescript@4.1.5...clasp login で Google OAuth 2.0 を使った認証が実施され、Authorization が成功すると ~/.clasprc.json というファイルにログイン情報が保存される。開発のためのセットアップ適当にディレクトリ（ここでは sample とする）を掘って npm init などをする（yarn がよかったらよしなに読み替える）。$ mkdir sample$ cd sample$ npm init -y$ tslint --initこのディレクトリで新しい script project を作成する。standalone の script project を作成する。https://github.com/google/clasp を見ればいいんだけど、ここでは script project のタイトルと root directory を設定している。後者を設定しておくとこの root directory に置いてあるものだけが push したときにスクリプトエディタに送られて便利なので設定しておくのがよい（これを設定しない場合は不必要なファイルを push しないように ./.claspignore を作る必要がある）。$ clasp create --title \"sample-title\" --rootDir ./src ? Create which script? standaloneGAS API を使ったことがなければ初回はエラーが出るので、https://script.google.com/home/usersettings で Google Apps Script API をオンにする必要がある。セットアップが整ったことを確認するために、簡単な GAS を作って push して実行してみる。./src/sample.ts を以下のように準備する。function test(): void {    console.log(\"hellow world.\");}このファイルを以下のコマンドで Google Drive に push する。./src/ 以下のファイルが push される。.ts ファイルが .gs ファイルにトランスパイルされて、GAS として実行できるようになる。$ clasp push└─ src/appsscript.json└─ src/sample.tspush されたファイルは GAS として Google Drive に保存される。GAS をスクリプトエディタ（script.google.com）で開くには以下を実行する。$ clasp openこれで以下の図のようにスクリプトエディタが開く。// Compiled using ts2gas 3.6.4 (TypeScript 4.1.5) とか書いてあってトランスパイルされたことも確認できる。型アノテーションも落ちている。実行 すればよいという寸法である。Google Drive に GAS として保存されたコードをローカルに持ってくるには clasp pull をすればよい。これをすると .gs ファイルから .js ファイルにトランスパイルされてローカルの ./src/ 以下に保存される。pull するとローカルに同じ内容の .ts ファイルと .js ファイルが存在するようになってしまうので、再び push すると 400 エラーで A file with this name already exists in the current project: sample となるので、push するには .js　ファイルを削除する必要がある。ローカルで開発する場合は pull は殆ど使わない、ということになりそう。./clasp.json に \"fileExtension\": \"ts\" を追加しておくと .ts ファイルとして pull してくれるが、当然型アノテーションとかは破棄されてるので嬉しいことは特にない。）これだけでもやりたいことはやれなくはないが、コードを動かすために push してから毎回ブラウザを開いて実行ボタンを押下しないといけないのでしんどい。App Script のランタイムは Google の基盤にあるので push して実行しないといけないのはいいのだが、せめてローカルでコマンドを叩いて実行してその結果をローカルで確認する、くらいはしたい。それをするためには色々と面倒な準備をしないといけないので、以降ではそのやり方を記録していく。ローカルで clasp run が実行できるようにするこれがダルい。かなりダルいのでもっといい感じにできるようになって欲しいが、とりあえず自分がやった手順を残しておく。何もしない状態で clasp run を実行するとどうなるかをまず見ておく。clasp run を実行するとどの関数を実行するか選択できるので、今回であれば test を選ぶ。$ clasp runRunning in dev mode.? Select a functionName testCould not read API credentials. Are you logged in locally?こんな感じで API credentials が読めないと怒られる。これを解決するために具体的な手順として以下を実施することになる。GCP プロジェクトと連携GAS を実行可能 API として公開App Script API を有効化認証情報の作成とそれを用いたログインGCP プロジェクトと連携するにはスクリプトエディタで プロジェクトの設定 を選択し、GCP の プロジェクトの変更 から連携させたい GCP プロジェクトのプロジェクト番号を設定すればよい。実行可能 API として公開するには、スクリプトエディタの デプロイ \u003e 新しいデプロイ において 実行可能API としてアクセスできるユーザをお望みのものに設定した上でデプロイする。この段階で clasp pull すると ./src/appsscript.json に executionAPI が追加されることが確認できる。{  \"timeZone\": \"America/New_York\",  \"dependencies\": {},  \"exceptionLogging\": \"STACKDRIVER\",  \"runtimeVersion\": \"V8\",  \"executionApi\": {    \"access\": \"MYSELF\"  }}GCP のプロジェクトで APIとサービス \u003e ライブラリ で Apps Script API を検索して有効にすることもやっておく（もし過去に有効にしたことがなければ）。最後に認証情報を作成してそれを使ってログインをする。APIとサービス \u003e 認証情報 の 認証情報を作成 で、OAuth クライアント ID を デスクトップアプリ アプリケーション用に作成する。この認証情報を json としてダウンロード（以降ではこのファイル名を creds.json とする）して適当な場所に置く（ここでは sample ディレクトリ直下に置いたものとする）。? What is your GCP projectId? と聞かれるので GCP のプロジェクト ID を入力する。$ clasp login --creds creds.json これが成功すると、sample ディレクトリ以下にこのプロジェクトで使用するアクセストークンなどが記載された .clasprc.json というファイルが作成される。ここまで来れば clasp run が実行できるようになるのでやってみる（以下のように実行したい関数名を指定して実行することができる）。$ clasp run testRunning in dev mode.No response.ということで、動く！clasp run では返り値が表示されることになるので console.log() とかの結果は表示されない（これに関しては次の節で書く）。適当な string とかを return すればちゃんと結果が表示されることが確認できる。ログの確認ログは GCP の Cloud Logging で管理されている。clasp のコマンドでローカルでも確認できるようになっている。$ clasp logsただし、これは連携している GCP プロジェクトに関するログがそのまま表示されてしまうので、そのプロジェクトが色々な用途で使われているものだとお望みの GAS のログを見つけるのが難しい。GCP のログエクスプローラだとフィルタリングのためのクエリが発行できるのでそれを clasp でも使えるとよいのだけど、20210223 時点ではサポートされていない。諦めてログエクスプローラで以下のようなクエリ（その他必要な条件は適宜つけて）を発行して見るしかないかな。resource.type=\"app_script_function\"resource.labels.invocation_type=\"apps script api\"GAS のサービスを使うここまで長々とやってきたが、GAS を使うなら GAS のサービスを使いたいわけで、それをするには使いたいサービスに応じて適切な permission を設定する必要がある。これは必要なものを ./src/appsscript.json に記載しましょうという話なのだが、せっかくなので一つ例を。先ほどまで使っていた ./src/test.ts を以下のように書き換える。function test(): string {    var response = UrlFetchApp.fetch('https://httpbin.org/get');    return response.getContentText();}これを実行しようとすると以下のようなエラーが出る。$ clasp push \u0026\u0026 clasp run testException: ScriptError Exception: You do not have permission to call UrlFetchApp.fetch. Required permissions: https://www.googleapis.com/auth/script.external_request [ { function: 'test', lineNumber: 3 } ]この permission を ./src/appsscript.json に加える。{  \"timeZone\": \"America/New_York\",  \"dependencies\": {},  \"exceptionLogging\": \"STACKDRIVER\",  \"runtimeVersion\": \"V8\",  \"executionApi\": {    \"access\": \"MYSELF\"  },  \"oauthScopes\": [    \"https://www.googleapis.com/auth/script.external_request\"  ]}これを追加したら改めてログインして再度実行する。 ? Manifest file has been updated. Do you want to push and overwrite? と聞かれるので y で答えると変更した appscript.json が push される。うまくいけば以下のようにちゃんと結果が帰ってくる。$ clasp login --creds...$ clasp push \u0026\u0026 clasp run test...Running in dev mode.{  \"args\": {},   \"headers\": {    \"Accept-Encoding\": \"gzip,deflate,br\",     \"Host\": \"httpbin.org\",     ...  },   ...  \"url\": \"https://httpbin.org/get\"}これくらいまでできるようになれば、何かしらのサービスの API を叩いてちょっとした処理をするのを GAS で定期実行するコードを書く、とかもローカルでできるようになってめでたしめでたし。その他clasp run foo で関数 foo を実行するときに引数を渡すこともできるが、これはシングルクォーテーションで囲んで例えば string なら clasp run foo --params '\"paramString\"' みたいに書く必要があるので注意。まあ README https://github.com/google/clasp/tree/ee70a6ae743d3b28b27b2c5cc6ca3e70c64f465b#options-11 とかコードをちゃんと読めってだけの話なんだけど。npm パッケージを使いたい場合はどうするんだっけというのもあるが、これはローカルでビルドして push せいという話みたいなので自分にも必要性が発生したら調べたらよさそう。それ以外にも設定ファイルの細々したこととかあるけど、大した話じゃないので割愛。テストとかデバッグとかGAS を使いたいときは GAS のサービスを使いたいときが多いので、やはりローカルだと限界があって、結局はスクリプトエディタでデバッガを使いながら期待通りの動きをしているのか確認する、というのが試行錯誤しながら開発するときは効率的ということになりそう。ある程度しっかり開発するなら、ローカルでモックを準備してテストもできるようにして、とか複数人で開発できるようにして、とかで今回のような環境をちゃんと作った上でやっていくのがよいと思うけど、自分の用途としてはそこまでガッツリしたものを GAS で作ることはなかなかなさそう。ということで、自分の用途に限れば、黙ってスクリプトエディタで開発して出来たコードは gist なりなんなりで共有しておけば十分、という感じかもしれない。「そしたらなんのためにローカルで TypeScript で書いたの」だって？せっかくだからちょっとやってみようと思ったからだよ、こまけぇこたぁいいんだよ。（AA略オチがついたところでこのエントリは終了。参考にした情報文中で書いたものもあるがまとめて羅列しておく。clasp: https://github.com/google/clasphttps://developers.googleblog.com/2015/12/advanced-development-process-with-apps.htmlhttps://developers.google.com/apps-script/referencehttps://developers.google.com/apps-script/manifesthttps://developers.google.com/apps-script/api/how-tos/executehttps://developers.google.com/apps-script/reference/url-fetch/url-fetch-appまとめclasp + TypeScript でローカルで GAS の開発ができる環境を整えたという話。","link":"https://yoheikikuta.github.io/GAS_local_development_env/","isoDate":"2021-02-22T00:00:00.000Z","dateMiliSeconds":1613952000000,"authorName":"yoheikikuta","authorId":"yoheikikuta"},{"title":"2020年のまとめ","contentSnippet":"TL;DR2020 年も頑張った（4 年連続 4 回目）。4 月に社会復帰して途中で折れずに働き続けた超人として自分を褒め称える2021 年も頑張っていきますか〜！！！2020年が終わった。 全体的によく頑張った。毎年頑張っていて偉い。本当に偉い。凄い（去年のをコピペ）（去年のをコピペ）（去年のをコピペ）。このブログを始めて 4 回目の一年振り返りエントリで、これくらいの数になってくるとやっとるな感が出てくるね。無職をやめて社会復帰（就職）した甘美な無職期間に終わりを告げ、社会復帰を果たした。無職期間は 20190201 ~ 20200331 だった。最後の方は結婚式費用のために現金を稼がねばならずフリーランス業をいくらかやったが、結婚式は延期になったのでノーカンで（謎理論）。無職期間に何をしていたかは去年の年末ブログエントリ https://yoheikikuta.github.io/Summary2019/ に色々書いたのでそちらに譲るとして、無職最高なのでみんなやった方がいいぞということだけここで改めて言っておきたい。沖縄に行って海辺をランニングして仕事に関係ない数学書とか哲学書を読んだり、海外に遊びに行って知り合いの家に居候したり、なにも考えずにただひたすら漫画を読んだり、やりたいことを元気があるうちにやっとくのはいいな〜ということを満喫できる。また、仕事についてちゃんと考え直す機会としても良かった。仕事をしているときは「自分がやっている仕事は面白いし意義がある」と思い込む傾向がある。これは必ずしも悪いことではないけど、意識的に思考を切り替えないと自分が普段やっている仕事の枠内でしか物を考えなくなりがちなので、思考の切り替えのためにも無職期間は適している。ということで次に何をするかについては色々と考えて、多くの会社の話を聞いたり実際に受けたりして就職先を検討した。人に語れるようなストーリー（真に価値があると信じることができて、アルゴリズムやデータが本質的に重要で、得られる知見に普遍性があり、インパクトも大きい）があって、経済的にも成功できる（所得税は高すぎるので SO も有力な選択肢）、という観点を大事にした。最終的に Ubie 株式会社という会社に入社した。その時の経緯はブログエントリ https://yoheikikuta.github.io/joinning_ubie/ にも書いたので、詳しくはこちらで。Ubie での仕事「人に語れるように、とかなんとか言っておいて君は仕事のこと全然語ってなくない？」自分のことを客観的に見たらこういう疑問を抱く。やめてくれカカシ、その術はオレに(ry…どういう感じで働いてるのか、についてはまた別のブログエントリで（できれば）詳しく書きたいと思っているが、業種的に内部での取り組みを大っぴらに話しづらいという傾向がある。法的に未整備な領域も多いので慎重に進める必要があるし、データは知見の山だが内容を公開となるとやはり慎重を期す必要がある（どうしてもデータにはエラーやノイズが含まれるし、万一それによって不利益を被る人がいたときに深刻な問題となり得る）。この辺りは入社してからそういえばそうだな〜と気付かされた部分で、不満を感じている部分ではある。組織の取り組みとかは公開してるしそれはそれで面白いし大事だけど、自分がアウトプットするならもっと普段の業務内容に近い技術的な内容がいいなと思っているので、これまで会社での仕事のことはほとんどアウトプットできていなかった。やりたいけどなかなか変えづらい部分でもあるので、外向けに色々話すための個人的なアウトプットの場として Podcast を始めたりした（後述）。それ以外に関してはかなり満足して働けている。「”あの” 菊田さんが活き活き働いている」ということで Ubie に転職決めてくれた人も何名かいらっしゃいます。基本的にどの会社で働いている時も活き活き働いていたはずなのですが、ともかく話を聞いてみたいという人がいればお気軽に連絡ください！個人的に話す分には語りまくれるので語りまくります。業務内容をめちゃくちゃ雑に書くと、データサイエンスチームでデータ周りの諸々やアルゴリズムの改善などを主たる業務にしている。今年は特に「いかにして質の高いデータを集めてそれを使ってサービスを改善するか」という部分に注力していて、技術的に面白味がない部分も大量にあるけどデータが面白いのでめちゃくちゃ頑張った。かなり良い感じになってきたので、来年はアルゴリズムの改善でいろいろなアイデアを試せそうで楽しみ。何はともあれ 20200401 に入社して以来ここまで働いてこれたので、ただただ自分を褒めるのみである。超人だよ自分は（そして同様に働いている他の人々も）。インプット/アウトプットのために hikifune.fm という Podcast を始めた始まりました。前述の通り、技術的な内容をアウトプットする場がないので、新しい取り組みとして Podcast を始めた。そのことを書いたブログエントリはこちら https://yoheikikuta.github.io/podcast_hikifunefm/コンセプトは自分たちの勉強のため、というものなので、基本的にゲストを呼んで話してもらうとかはしないし内容も自分たちの興味があるものだけを対象にしている。隔週で配信しているが、これくらいが負担が少なくてよさそうな感じがしている。再生回数がどれくらいかというと以下の通り。最近リスナーが減っている感じもあるが、まあこんなもんかという感じ。内容が内容だし、時間も長めなので、平均して 100 人以上も聴いている人がいるというのは十二分な感じがしている。Podcast の取り組みについては概ね満足しているけど、ちょっとトピックがつまみ食い的な感じもあるので、自分が発表する番に関してはもう少しテーマを持って何回か連続してやってもいいかな〜などと考えている。Work From Home (WFH)今年は WFH が自分の中でスタンダードになったというのは大きな出来事だった。これまで働くんだったらやっぱり環境が整っているオフィスに行って議論とかできた方がいいよな、と思っていたけど、そうも言ってられない状況になったので WFH で満足いく働き方を実現しようと思って色々と頑張った。最終的に在宅勤務環境がこんな感じになったというブログエントリはこちら https://yoheikikuta.github.io/work_from_home_environment/正直引越し前はコワーキングスペースを借りたりなんだりしてたけど、かなり生産性が低かった。転職したてで色々慣れていなかったということもあったけど、あの頃はあまり価値を発揮できていなかったのでよくなかったなと反省。真面目に WFH 環境を整えてからは驚くほど生産性が上がった。ホワイトボードを使って人と議論する時以外は全部 WFH でいいじゃん、と思っている。WFH 最高だし、物理出社しないと原理的に業務ができない人たちのため（感染リスクを下げる）にも、WFH で働ける人はみんなそうすべきだよねと思う程度には推進派になった。WFH で肩凝りに悩まされることがなくなったというのも良かった点。スタンディングデスクやらの設備投資と、家だと好きなだけ動き回れるという利点も相まって、肩凝りはかなり軽減した。ゲームやるときにずっと座ってやってて肩凝るので、一番の難敵はゲームだと理解した。反省点は圧倒的運動不足。これは来年の目標に明示的に入れてやっていこう。運動不足とは直接関係ないが、今年はピロリ菌感染疑いで胃痛が発生したりして健康を意識した一年でもあった。除菌をしてある程度収まったのでいまは大丈夫だが、やはり健康第一なので運動含めてちゃんと体調管理していこうと思った次第。その他毎年元旦にその年の目標を書いて（プライベートな内容も多いので公開してない）ちょこちょこチェックしてるけど、一年経って見返してみるとアウトプットの量と運動の項目に関しては未達が多かった。アウトプット量に関しては、会社でかなりの量のテキストを書いてるのでそれで満足しちゃっているという面があるので、大きく変えるというのはなかなか難しいかもしれない。運動に関しては一番の課題に感じてるので、これは意識して改善しよう。体重とかはほとんど変化してないけど、あからさまに体力が落ちてる気がする（気がする、は運動をしてなさすぎてそれを自覚するイベントがない）。PS5 を買えたのは良かった。グラフィックが素晴らしくてデモンズソウルを一からのやり直し含めて 5 周以上プレーした。来年以降色々とソフトも出るだろうし、楽しみ。Web 上での存在感は薄くなっているので、そこはもう少し頑張っていきたいかな。Twitter とか全然できてないし。Trickle の year stats は 912 activities, 279 days だったので、Twitter も同じくらいは頑張りたいところ。ということで、来年も頑張っていきましょう。まとめ2020 年も終わりです。みなさま良いお年を。","link":"https://yoheikikuta.github.io/Summary2020/","isoDate":"2020-12-30T00:00:00.000Z","dateMiliSeconds":1609286400000,"authorName":"yoheikikuta","authorId":"yoheikikuta"},{"title":"2020年に読んだ本を一言コメントと共に振り返る","contentSnippet":"TL;DR2020 年に読んだ本を可能な限り思い出して振り返ってみる詳しい書評を書くのはキツいので一言コメントを添えて仕事を始めるということで組織系の本が多め。技術書は少ないな〜2020 年に読んだ本を思い出せる分だけ一言コメントと共に振り返ってみる（記録を残してない書籍もあるが、それらは無視）。雑に技術書と読み物に分けて、雑多に書いていく。一言コメント、と言いながら結構な量のコメントな気もするが、気にせずいこう。技術書Google BigQuery The Definitive Guide amazon へのリンク入門ベイズ統計 意思決定の理論と発展 amazon へのリンク科学と証拠 ―統計の哲学 入門― amazon へのリンクベイズ推論による機械学習入門 amazon へのリンクPython の黒魔術 BOOTH へのリンク__get__(), __set__(), __delete__() のどれかを持ってるオブジェクト。 instance メソッドや class メソッドや static メソッドで何が行われているのかが理解できる。第11章の例外とトレースバック。sys.excepthook という例外ハンドラを自分で置き換えてトレースバックを変更したり表示カラー化をしてみるという章。トレースバックこうなってたのねというのが理解できるのがよかった。Visual Studio Code Ninja Guide BOOTH へのリンクGitHub Actions 実践入門 amazon へのリンクレガシーコードからの脱却 amazon へのリンクRust プログラミング入門 amazon へのリンクデータ指向アプリケーションデザイン amazon へのリンク読み物闘うプログラマー amazon へのリンク文芸的プログラミング amazon へのリンク独創はひらめかない amazon へのリンクThe Elements of Style (Fourth Edition) amazon へのリンクhttps://twitter.com/polm23/status/1343874246557110273 をいただいて気付けました）。この本に誤りが多いことは https://www.chronicle.com/article/50-years-of-stupid-grammar-advice などにも書いてあります。みずほ銀行システム統合、苦闘の 19 年史 amazon へのリンクMeasure What Matters amazon へのリンクLearn or Die 死ぬ気で学べ プリファードネットワークスの挑戦 amazon へのリンクHOW GOOGLE WORKS amazon へのリンク\b1兆ドルコーチ amazon へのリンクティール組織 amazon へのリンク1984年 amazon へのリンクホラクラシーの光と影 amazon へのリンクHolacracy: The New Management System for a Rapidly Changing World amazon へのリンクNO RULES amazon へのリンクまとめ仕事を始めて会社で色々と新しい組織的な取り組みをしているので、組織系の書籍を読むことがまあまあ多かった。","link":"https://yoheikikuta.github.io/book_I_read_in_2020/","isoDate":"2020-12-29T00:00:00.000Z","dateMiliSeconds":1609200000000,"authorName":"yoheikikuta","authorId":"yoheikikuta"}]},"__N_SSG":true},"page":"/members/[id]","query":{"id":"yoheikikuta"},"buildId":"s6ZnN9IX6Cd8IMoMBL1Ds","nextExport":false,"isFallback":false,"gsp":true,"head":[["meta",{"name":"viewport","content":"width=device-width"}],["meta",{"charSet":"utf-8"}],["link",{"rel":"icon shortcut","type":"image/png","href":"https://blog.ubie.tech/logo.png"}],["link",{"rel":"stylesheet","href":"https://fonts.googleapis.com/css2?family=Inter:wght@400;700\u0026display=swap"}],["title",{"children":"yoheikikuta | Ubie Engineers' Blogs"}],["meta",{"property":"og:title","content":"yoheikikuta"}],["meta",{"property":"og:url","content":"https://blog.ubie.tech/members/yoheikikuta"}],["meta",{"name":"twitter:card","content":"summary_large_image"}],["meta",{"property":"og:site","content":"Ubie Engineers' Blogs"}],["meta",{"property":"og:image","content":"https://blog.ubie.tech/og.png"}],["link",{"rel":"canonical","href":"https://blog.ubie.tech/members/yoheikikuta"}]]}</script><script nomodule="" src="/_next/static/chunks/polyfills-fa276ba060a4a8ac7eef.js"></script><script src="/_next/static/chunks/main-8a83f0fd99327c4684a8.js" async=""></script><script src="/_next/static/chunks/webpack-e067438c4cf4ef2ef178.js" async=""></script><script src="/_next/static/chunks/framework.1daf1ec1ecf144ee9147.js" async=""></script><script src="/_next/static/chunks/commons.8d61253ae98ee51657b8.js" async=""></script><script src="/_next/static/chunks/pages/_app-49079e3278dd6cef7229.js" async=""></script><script src="/_next/static/chunks/81b50c7ab23905e464b4340eb234bd6ea389d26b.1ba37b316530e91fa3ed.js" async=""></script><script src="/_next/static/chunks/pages/members/%5Bid%5D-f279413a3daf3c18264d.js" async=""></script><script src="/_next/static/s6ZnN9IX6Cd8IMoMBL1Ds/_buildManifest.js" async=""></script><script src="/_next/static/s6ZnN9IX6Cd8IMoMBL1Ds/_ssgManifest.js" async=""></script></body></html>